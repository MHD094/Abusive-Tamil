{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10503977,"sourceType":"datasetVersion","datasetId":6456866}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\nimport os\nfrom glob import glob\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport re\nimport nltk\nimport json\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\nfrom sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nnp.random.seed(42)\nimport nltk\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import ToktokTokenizer\nimport nltk, string, re, spacy,unicodedata, random\nfrom bs4 import BeautifulSoup\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:09:50.792539Z","iopub.execute_input":"2025-01-29T17:09:50.792905Z","iopub.status.idle":"2025-01-29T17:09:50.802179Z","shell.execute_reply.started":"2025-01-29T17:09:50.792881Z","shell.execute_reply":"2025-01-29T17:09:50.801306Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nCPU times: user 1.16 ms, sys: 0 ns, total: 1.16 ms\nWall time: 1.11 ms\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/abusive/AWT_train.csv\")\ndev_df = pd.read_csv(\"/kaggle/input/abusive/AWT_dev.csv\")\ntest_df = pd.read_csv('/kaggle/input/abusive/AWT_test_with_labels.csv')\n\nprint(\"Total number of training samples:\", len(train_df))\nprint(\"Total number of dev samples:\", len(dev_df))\nprint(\"Total number of test samples:\", len(test_df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:09:53.906638Z","iopub.execute_input":"2025-01-29T17:09:53.906954Z","iopub.status.idle":"2025-01-29T17:09:53.996916Z","shell.execute_reply.started":"2025-01-29T17:09:53.906929Z","shell.execute_reply":"2025-01-29T17:09:53.996203Z"}},"outputs":[{"name":"stdout","text":"Total number of training samples: 2790\nTotal number of dev samples: 598\nTotal number of test samples: 598\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:09:56.341724Z","iopub.execute_input":"2025-01-29T17:09:56.341999Z","iopub.status.idle":"2025-01-29T17:09:56.357062Z","shell.execute_reply.started":"2025-01-29T17:09:56.341977Z","shell.execute_reply":"2025-01-29T17:09:56.356125Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                Text        Class\n0       இதல்லம் ஒரு தீர்ப்பு நாட்டாமை தீர்ப்பை மற்று  Non-Abusive\n1           யாருடா அந்த கார்த்தி, நீ எங்கடா இருக்க ?  Non-Abusive\n2  இரண்டு பேரின் (புண்டையை) சாமான்களை கோணிய ஊசால்...      Abusive\n3  என்ன திமிர் இந்த பொண்ணுக்கு.....மக்களே இன்னும்...      Abusive\n4          ஐயோ அந்த கார்த்திக் எ காட்டுங்க பா please  Non-Abusive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>இதல்லம் ஒரு தீர்ப்பு நாட்டாமை தீர்ப்பை மற்று</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>யாருடா அந்த கார்த்தி, நீ எங்கடா இருக்க ?</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>இரண்டு பேரின் (புண்டையை) சாமான்களை கோணிய ஊசால்...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>என்ன திமிர் இந்த பொண்ணுக்கு.....மக்களே இன்னும்...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ஐயோ அந்த கார்த்திக் எ காட்டுங்க பா please</td>\n      <td>Non-Abusive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"#train_df.duplicated().sum().item()\nprint(dev_df.duplicated().sum().item())\nprint(train_df.duplicated().sum().item())\ntest_df.duplicated().sum().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:09:58.744783Z","iopub.execute_input":"2025-01-29T17:09:58.745069Z","iopub.status.idle":"2025-01-29T17:09:58.766121Z","shell.execute_reply.started":"2025-01-29T17:09:58.745049Z","shell.execute_reply":"2025-01-29T17:09:58.765523Z"}},"outputs":[{"name":"stdout","text":"0\n11\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_df.drop_duplicates(inplace=True)\ndev_df.drop_duplicates(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:01.463316Z","iopub.execute_input":"2025-01-29T17:10:01.463675Z","iopub.status.idle":"2025-01-29T17:10:01.472947Z","shell.execute_reply.started":"2025-01-29T17:10:01.463650Z","shell.execute_reply":"2025-01-29T17:10:01.472174Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(dev_df.duplicated().sum().item())\ntrain_df.duplicated().sum().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:03.614296Z","iopub.execute_input":"2025-01-29T17:10:03.614643Z","iopub.status.idle":"2025-01-29T17:10:03.626025Z","shell.execute_reply.started":"2025-01-29T17:10:03.614618Z","shell.execute_reply":"2025-01-29T17:10:03.625107Z"}},"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Count labels\nnum_labels = train_df['Class'].value_counts()\nprint(num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:06.150042Z","iopub.execute_input":"2025-01-29T17:10:06.150439Z","iopub.status.idle":"2025-01-29T17:10:06.161404Z","shell.execute_reply.started":"2025-01-29T17:10:06.150405Z","shell.execute_reply":"2025-01-29T17:10:06.160393Z"}},"outputs":[{"name":"stdout","text":"Class\nNon-Abusive    1421\nAbusive        1357\nabusive           1\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Change class label from 'abusive' to 'Abusive'\ntrain_df['Class'] = train_df['Class'].replace('abusive', 'Abusive')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:09.379790Z","iopub.execute_input":"2025-01-29T17:10:09.380096Z","iopub.status.idle":"2025-01-29T17:10:09.384947Z","shell.execute_reply.started":"2025-01-29T17:10:09.380074Z","shell.execute_reply":"2025-01-29T17:10:09.384042Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Count labels\nnum_labels = train_df['Class'].value_counts()\nprint(num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:11.306523Z","iopub.execute_input":"2025-01-29T17:10:11.306805Z","iopub.status.idle":"2025-01-29T17:10:11.312884Z","shell.execute_reply.started":"2025-01-29T17:10:11.306787Z","shell.execute_reply":"2025-01-29T17:10:11.311885Z"}},"outputs":[{"name":"stdout","text":"Class\nNon-Abusive    1421\nAbusive        1358\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"filtered_train = train_df[lambda x: x[\"Text\"].str.contains(\"[A-Za-z0-9]\")]\nprint(filtered_train.shape)\nfiltered_dev = dev_df[lambda x: x[\"Text\"].str.contains(\"[A-Za-z0-9]\")]\nprint(filtered_dev.shape)\nfiltered_test = test_df[lambda x: x[\"Text\"].str.contains(\"[A-Za-z0-9]\")]\nprint(filtered_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:13.842452Z","iopub.execute_input":"2025-01-29T17:10:13.842805Z","iopub.status.idle":"2025-01-29T17:10:13.857952Z","shell.execute_reply.started":"2025-01-29T17:10:13.842778Z","shell.execute_reply":"2025-01-29T17:10:13.857157Z"}},"outputs":[{"name":"stdout","text":"(736, 2)\n(161, 2)\n(155, 3)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":" **Preprocessing**","metadata":{}},{"cell_type":"code","source":"def text_to_word_list(text):\n    text = text.split()\n    return text\n\ndef replace_strings(text):\n    emoj = re.compile(\"[\"         # this emoj is to remove all emojis\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\"\n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           u\"\\u00C0-\\u017F\"          #latin\n                           u\"\\u2000-\\u206F\"          #generalPunctuations\n\n                           \"]+\", flags=re.UNICODE)\n    #english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n\n    text=emoji_pattern.sub(r'', text)\n    text=emoj.sub(r'',text)\n    text=text.lower()\n    text=re.sub(r'\\s+', ' ', text)\n\n    text = BeautifulSoup(text, 'html.parser').get_text()\n    text = re.sub(r'(https|http|www)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', ' ', text, flags=re.MULTILINE)\n\n    text = text.replace('\\n', ' ')\n    text = text.replace('—', ' ')\n    text = text.replace('_', ' ')\n    text = text.replace('\\r', ' ')\n    text = re.sub(r'\\\\', ' ',text)\n    # Stopword Removing\n\n    #text=english_pattern.sub(r'', text)\n    #def remove_emojis(text):\n    #text= emoji.get_emoji_regexp().sub(r'', text)\n\n    return text\n\ndef remove_punctuations(my_str):\n    # define punctuation\n    punctuations = '''````£|¢|Ñ+-*/=৳০১২৩৪৫৬৭৮৯–•।!()-[]{};:'\"“\\’,<>./?@#$%^&*_~‘—॥”‰🤣⚽️✌😀😃😄😁😆😅😂🤣😊😇🙂🙃😉😌😍😘😗😙😚😋😛😝😜🤪🤨🧐🤓😎🤩🥳😏😒😞😔😟😕🙁☹️😣😖😫😩😤😠😡🤬😈👿💀☠️💩🤡👹👺👻👽👾🤖💋💌💘💝💖💗💓💞💕💟❣️💔❤️‍🔥🖤🤍💯💢💥💫💦💨🕳️💣💬👁️‍🗨️🗨️🗯️💭💤👋🤚🖐️✋🖖👌✌️🤞🤟🤘🤙👈👉👆👇☝️✍️👏👏🏻👏🏼👏🏽👏🏾👏🏿👍👍🏻👍🏼👍🏽👍🏾👍🏿👎👎🏻👎🏼👎🏽👎🏾👎🏿✊✊🏻✊🏼✊🏽✊🏾✊🏿🤛🤛🏻🤛🏼🤛🏽🤛🏾🤛🏿🤜🤜🏻🤜🏼🤜🏽🤜🏾🤜🏿🤝🙏🙏🏻🙏🏼🙏🏽🙏🏾🙏🏿✍️💅💅🏻💅🏼💅🏽💅🏾💅🏿🤳💪💪🏻💪🏼💪🏽💪🏾💪🏿🦾🦵🦿🦶👂👂🏻👂🏼👂🏽👂🏾👂🏿👃👃🏻👃🏼👃🏽👃🏾👃🏿👀🧠🫀🫁🦷🦴👅👄🦻🦻🏻🦻🏼🦻🏽🦻🏾🦻🏿👶👶🏻👶🏼👶🏽👶🏾👶🏿🧒🧒🏻🧒🏼🧒🏽🧒🏾🧒🏿👦👦🏻👦🏼👦🏽👦🏾👦🏿👧👧🏻👧🏼👧🏽👧🏾👧🏿🧑🧑🏻🧑🏼🧑🏽🧑🏾🧑🏿👨👨🏻👨🏼👨🏽👨🏾👨🏿👩👩🏻👩🏼👩🏽👩🏾👩🏿🧓🧓🏻🧓🏼🧓🏽🧓🏾🧓🏿👴👴🏻👴🏼👴🏽👴🏾👴🏿👵👵🏻👵🏼👵🏽👵🏾👵🏿👩‍🦰👩🏻‍🦰👩🏼‍🦰👩🏽‍🦰👩🏾‍🦰👩🏿‍🦰👩‍🦱👩🏻‍🦱👩🏼‍🦱👩🏽‍🦱👩🏾‍🦱👩🏿‍🦱👩‍🦳👩🏻‍🦳👩🏼‍🦳👩🏽‍🦳👩🏾‍🦳👩🏿‍🦳👩‍🦲👩🏻‍🦲👩🏼‍🦲👩🏽‍🦲👩🏾‍🦲👩🏿‍🦲👨‍🦰👨🏻‍🦰👨🏼‍🦰👨🏽‍🦰👨🏾‍🦰👨🏿‍🦰👨‍🦱👨🏻‍🦱👨🏼‍🦱👨🏽‍🦱👨🏾‍🦱👨🏿‍🦱👨‍🦳👨🏻‍🦳👨🏼👨🏽‍🦳👨🏾‍🦳👨🏿‍🦳👨‍🦲👨🏻‍🦲👨🏼‍🦲👨🏽‍🦲👨🏾‍🦲👨🏿‍🦲🦰🦱🦳🦲👱‍♀️👱🏻‍♀️❤️‍🩹❤️‍🔥❤️‍🩹🧡💛💚💙💜🤎�￰৷￰'''\n    #punctuations = '|¢|Ñ+-৳০১২৩৪৫৬৭৮৯।()-[]{}<>@#$%^&*_~—॥🤣⚽️✌😀💉�￰৷￰'\n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuations:\n            no_punct = no_punct + char\n\n    # display the unpunctuated string\n    return no_punct\n\n\n\ndef joining(text):\n    out=' '.join(text)\n    return out\n\ndef preprocessing(text):\n    out=remove_punctuations(replace_strings(text))\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:17.955423Z","iopub.execute_input":"2025-01-29T17:10:17.955722Z","iopub.status.idle":"2025-01-29T17:10:17.964220Z","shell.execute_reply.started":"2025-01-29T17:10:17.955702Z","shell.execute_reply":"2025-01-29T17:10:17.963405Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Apply preprocessing to all datasets\ntrain_df['cleanText'] = train_df['Text'].apply(lambda x: preprocessing(str(x)))\ntest_df['cleanText'] = test_df['Text'].apply(lambda x: preprocessing(str(x)))\ndev_df['cleanText'] = dev_df['Text'].apply(lambda x: preprocessing(str(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:22.307810Z","iopub.execute_input":"2025-01-29T17:10:22.308109Z","iopub.status.idle":"2025-01-29T17:10:22.839950Z","shell.execute_reply.started":"2025-01-29T17:10:22.308075Z","shell.execute_reply":"2025-01-29T17:10:22.839255Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:27.120674Z","iopub.execute_input":"2025-01-29T17:10:27.120959Z","iopub.status.idle":"2025-01-29T17:10:27.131752Z","shell.execute_reply.started":"2025-01-29T17:10:27.120939Z","shell.execute_reply":"2025-01-29T17:10:27.130626Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"      id                                               Text        Class  \\\n0      1  இவ ஒரு மானெங்கெட்ட பொறுக்கி. ஒரே ஒரு routine ஒ...      Abusive   \n1      2  இப்டியே பேசிக்கிட்டே இருந்தா எப்டி..... யாரு ப...  Non-Abusive   \n2      3  அடக் கடவுளே இது என்னக் கொடுமையை ஊருல உலகத்துல ...  Non-Abusive   \n3      4  இதற்கு ஒரு தீர்வு இருக்கு.  அவன் அவன் வேலை அவன...  Non-Abusive   \n4      5  தம்பி போய் நல்லவங்களை பேட்டிஎடு அவ சொல்வது அத்...      Abusive   \n..   ...                                                ...          ...   \n593  594          இது பைதியமா இல்லை நாம பைதியமா முடியல சாமி  Non-Abusive   \n594  595     இரண்டு வல்கர் சாக்கடை. கட்டி வச்சி அடிக்கணும்.      Abusive   \n595  596  உன்கிட்ட தப்பிச்ச கார்த்தி சந்தோசமா இருக்கான்....  Non-Abusive   \n596  597       கார்த்தி அண்ட் திவ்யா ஒரு ஷோ பண்ணுங்க வெயிட்  Non-Abusive   \n597  598  சுகந்திக்கு கார்த்தி இதுக்கும் என்ன சம்பந்தம் ...  Non-Abusive   \n\n                                             cleanText  \n0    இவ ஒரு மானெங்கெட்ட பொறுக்கி ஒரே ஒரு routine ஒர...  \n1    இப்டியே பேசிக்கிட்டே இருந்தா எப்டி யாரு பெருசு...  \n2    அடக் கடவுளே இது என்னக் கொடுமையை ஊருல உலகத்துல ...  \n3    இதற்கு ஒரு தீர்வு இருக்கு அவன் அவன் வேலை அவன் ...  \n4    தம்பி போய் நல்லவங்களை பேட்டிஎடு அவ சொல்வது அத்...  \n..                                                 ...  \n593          இது பைதியமா இல்லை நாம பைதியமா முடியல சாமி  \n594       இரண்டு வல்கர் சாக்கடை கட்டி வச்சி அடிக்கணும்  \n595  உன்கிட்ட தப்பிச்ச கார்த்தி சந்தோசமா இருக்கான் ...  \n596       கார்த்தி அண்ட் திவ்யா ஒரு ஷோ பண்ணுங்க வெயிட்  \n597  சுகந்திக்கு கார்த்தி இதுக்கும் என்ன சம்பந்தம் ...  \n\n[598 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Text</th>\n      <th>Class</th>\n      <th>cleanText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>இவ ஒரு மானெங்கெட்ட பொறுக்கி. ஒரே ஒரு routine ஒ...</td>\n      <td>Abusive</td>\n      <td>இவ ஒரு மானெங்கெட்ட பொறுக்கி ஒரே ஒரு routine ஒர...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>இப்டியே பேசிக்கிட்டே இருந்தா எப்டி..... யாரு ப...</td>\n      <td>Non-Abusive</td>\n      <td>இப்டியே பேசிக்கிட்டே இருந்தா எப்டி யாரு பெருசு...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>அடக் கடவுளே இது என்னக் கொடுமையை ஊருல உலகத்துல ...</td>\n      <td>Non-Abusive</td>\n      <td>அடக் கடவுளே இது என்னக் கொடுமையை ஊருல உலகத்துல ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>இதற்கு ஒரு தீர்வு இருக்கு.  அவன் அவன் வேலை அவன...</td>\n      <td>Non-Abusive</td>\n      <td>இதற்கு ஒரு தீர்வு இருக்கு அவன் அவன் வேலை அவன் ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>தம்பி போய் நல்லவங்களை பேட்டிஎடு அவ சொல்வது அத்...</td>\n      <td>Abusive</td>\n      <td>தம்பி போய் நல்லவங்களை பேட்டிஎடு அவ சொல்வது அத்...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>594</td>\n      <td>இது பைதியமா இல்லை நாம பைதியமா முடியல சாமி</td>\n      <td>Non-Abusive</td>\n      <td>இது பைதியமா இல்லை நாம பைதியமா முடியல சாமி</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <td>595</td>\n      <td>இரண்டு வல்கர் சாக்கடை. கட்டி வச்சி அடிக்கணும்.</td>\n      <td>Abusive</td>\n      <td>இரண்டு வல்கர் சாக்கடை கட்டி வச்சி அடிக்கணும்</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>596</td>\n      <td>உன்கிட்ட தப்பிச்ச கார்த்தி சந்தோசமா இருக்கான்....</td>\n      <td>Non-Abusive</td>\n      <td>உன்கிட்ட தப்பிச்ச கார்த்தி சந்தோசமா இருக்கான் ...</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>597</td>\n      <td>கார்த்தி அண்ட் திவ்யா ஒரு ஷோ பண்ணுங்க வெயிட்</td>\n      <td>Non-Abusive</td>\n      <td>கார்த்தி அண்ட் திவ்யா ஒரு ஷோ பண்ணுங்க வெயிட்</td>\n    </tr>\n    <tr>\n      <th>597</th>\n      <td>598</td>\n      <td>சுகந்திக்கு கார்த்தி இதுக்கும் என்ன சம்பந்தம் ...</td>\n      <td>Non-Abusive</td>\n      <td>சுகந்திக்கு கார்த்தி இதுக்கும் என்ன சம்பந்தம் ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>598 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"pip install indic-transliteration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:32.618173Z","iopub.execute_input":"2025-01-29T17:10:32.618524Z","iopub.status.idle":"2025-01-29T17:10:37.609264Z","shell.execute_reply.started":"2025-01-29T17:10:32.618497Z","shell.execute_reply":"2025-01-29T17:10:37.607893Z"}},"outputs":[{"name":"stdout","text":"Collecting indic-transliteration\n  Downloading indic_transliteration-2.3.69-py3-none-any.whl.metadata (1.4 kB)\nCollecting backports.functools-lru-cache (from indic-transliteration)\n  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from indic-transliteration) (2024.9.11)\nRequirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from indic-transliteration) (0.12.5)\nRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from indic-transliteration) (0.10.2)\nCollecting roman (from indic-transliteration)\n  Downloading roman-5.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer->indic-transliteration) (8.1.7)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from typer->indic-transliteration) (4.12.2)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->indic-transliteration) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer->indic-transliteration) (13.8.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\nDownloading indic_transliteration-2.3.69-py3-none-any.whl (155 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\nDownloading roman-5.0-py3-none-any.whl (5.5 kB)\nInstalling collected packages: roman, backports.functools-lru-cache, indic-transliteration\nSuccessfully installed backports.functools-lru-cache-2.0.0 indic-transliteration-2.3.69 roman-5.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"**Converting Code-mix Text into Tamil Text**","metadata":{}},{"cell_type":"code","source":"import unicodedata\nfrom indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\n\n# Function for Unicode normalization and transliteration\ndef normalize_and_transliterate(text):\n    try:\n        # Step 1: Normalize Unicode\n        normalized = unicodedata.normalize('NFKC', text)\n        \n        # Step 2: Transliterate English words to Tamil\n        transliterated = transliterate(normalized, sanscript.ITRANS, sanscript.TAMIL)\n        \n        return transliterated\n    except Exception as e:\n        print(f\"Error processing text: {e}\")\n        return text  # Return original text if any error occurs\n\n# Apply the normalization and transliteration to the 'cleanText' column\ntrain_df['cleanText'] = train_df['cleanText'].apply(normalize_and_transliterate)\ndev_df['cleanText'] = dev_df['cleanText'].apply(normalize_and_transliterate)\ntest_df['cleanText'] = test_df['cleanText'].apply(normalize_and_transliterate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:01.353691Z","iopub.execute_input":"2025-01-29T17:11:01.353991Z","iopub.status.idle":"2025-01-29T17:11:02.286490Z","shell.execute_reply.started":"2025-01-29T17:11:01.353970Z","shell.execute_reply":"2025-01-29T17:11:02.285806Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:07.146224Z","iopub.execute_input":"2025-01-29T17:11:07.146576Z","iopub.status.idle":"2025-01-29T17:11:07.157450Z","shell.execute_reply.started":"2025-01-29T17:11:07.146549Z","shell.execute_reply":"2025-01-29T17:11:07.156617Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                                   Text        Class  \\\n0          இதல்லம் ஒரு தீர்ப்பு நாட்டாமை தீர்ப்பை மற்று  Non-Abusive   \n1              யாருடா அந்த கார்த்தி, நீ எங்கடா இருக்க ?  Non-Abusive   \n2     இரண்டு பேரின் (புண்டையை) சாமான்களை கோணிய ஊசால்...      Abusive   \n3     என்ன திமிர் இந்த பொண்ணுக்கு.....மக்களே இன்னும்...      Abusive   \n4             ஐயோ அந்த கார்த்திக் எ காட்டுங்க பா please  Non-Abusive   \n...                                                 ...          ...   \n2785  ரெண்டுமே அவனா இல்ல அவளா....ஒரே கன்ஃப்யூசன்.......      Abusive   \n2786  ராஜேஷ்வரி செம்மயா இருக்கு ரொம்ப குளோசப்ல பார்க...      Abusive   \n2787  எனக்கு தெறிச்சு கார்த்தி னு ஒருத்தன் நல்லா ஓத்...      Abusive   \n2788  நீங்கள் கவலை கொல்லாதீர்கள் சகோதரி. சின்மை எவ்ள...      Abusive   \n2789  30 lakhs views and 9k comments அட பாவிங்களா அவ...  Non-Abusive   \n\n                                              cleanText  \n0          இதல்லம் ஒரு தீர்ப்பு நாட்டாமை தீர்ப்பை மற்று  \n1                யாருடா அந்த கார்த்தி நீ எங்கடா இருக்க   \n2     இரண்டு பேரின் புண்டையை சாமான்களை கோணிய ஊசால் த...  \n3     என்ன திமிர் இந்த பொண்ணுக்குமக்களே இன்னும் இவளு...  \n4            ஐயோ அந்த கார்த்திக் எ காட்டுங்க பா ப்லேஅஸே  \n...                                                 ...  \n2785  ரெண்டுமே அவனா இல்ல அவளாஒரே கன்ஃப்யூசன் கஷ்டம் ...  \n2786  ராஜேஷ்வரி செம்மயா இருக்கு ரொம்ப குளோசப்ல பார்க...  \n2787  எனக்கு தெறிச்சு கார்த்தி னு ஒருத்தன் நல்லா ஓத்...  \n2788  நீங்கள் கவலை கொல்லாதீர்கள் சகோதரி சின்மை எவ்ளோ...  \n2789  ௩௦ லக்ஸ் விஏவ்ஸ் அந்த் ௯க் சோம்மேந்த்ஸ் அட பாவ...  \n\n[2779 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Class</th>\n      <th>cleanText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>இதல்லம் ஒரு தீர்ப்பு நாட்டாமை தீர்ப்பை மற்று</td>\n      <td>Non-Abusive</td>\n      <td>இதல்லம் ஒரு தீர்ப்பு நாட்டாமை தீர்ப்பை மற்று</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>யாருடா அந்த கார்த்தி, நீ எங்கடா இருக்க ?</td>\n      <td>Non-Abusive</td>\n      <td>யாருடா அந்த கார்த்தி நீ எங்கடா இருக்க</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>இரண்டு பேரின் (புண்டையை) சாமான்களை கோணிய ஊசால்...</td>\n      <td>Abusive</td>\n      <td>இரண்டு பேரின் புண்டையை சாமான்களை கோணிய ஊசால் த...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>என்ன திமிர் இந்த பொண்ணுக்கு.....மக்களே இன்னும்...</td>\n      <td>Abusive</td>\n      <td>என்ன திமிர் இந்த பொண்ணுக்குமக்களே இன்னும் இவளு...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ஐயோ அந்த கார்த்திக் எ காட்டுங்க பா please</td>\n      <td>Non-Abusive</td>\n      <td>ஐயோ அந்த கார்த்திக் எ காட்டுங்க பா ப்லேஅஸே</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2785</th>\n      <td>ரெண்டுமே அவனா இல்ல அவளா....ஒரே கன்ஃப்யூசன்.......</td>\n      <td>Abusive</td>\n      <td>ரெண்டுமே அவனா இல்ல அவளாஒரே கன்ஃப்யூசன் கஷ்டம் ...</td>\n    </tr>\n    <tr>\n      <th>2786</th>\n      <td>ராஜேஷ்வரி செம்மயா இருக்கு ரொம்ப குளோசப்ல பார்க...</td>\n      <td>Abusive</td>\n      <td>ராஜேஷ்வரி செம்மயா இருக்கு ரொம்ப குளோசப்ல பார்க...</td>\n    </tr>\n    <tr>\n      <th>2787</th>\n      <td>எனக்கு தெறிச்சு கார்த்தி னு ஒருத்தன் நல்லா ஓத்...</td>\n      <td>Abusive</td>\n      <td>எனக்கு தெறிச்சு கார்த்தி னு ஒருத்தன் நல்லா ஓத்...</td>\n    </tr>\n    <tr>\n      <th>2788</th>\n      <td>நீங்கள் கவலை கொல்லாதீர்கள் சகோதரி. சின்மை எவ்ள...</td>\n      <td>Abusive</td>\n      <td>நீங்கள் கவலை கொல்லாதீர்கள் சகோதரி சின்மை எவ்ளோ...</td>\n    </tr>\n    <tr>\n      <th>2789</th>\n      <td>30 lakhs views and 9k comments அட பாவிங்களா அவ...</td>\n      <td>Non-Abusive</td>\n      <td>௩௦ லக்ஸ் விஏவ்ஸ் அந்த் ௯க் சோம்மேந்த்ஸ் அட பாவ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2779 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"test_df['Class'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:13.618378Z","iopub.execute_input":"2025-01-29T17:11:13.618662Z","iopub.status.idle":"2025-01-29T17:11:13.625384Z","shell.execute_reply.started":"2025-01-29T17:11:13.618643Z","shell.execute_reply":"2025-01-29T17:11:13.624369Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Class\nAbusive        305\nNon-Abusive    293\nName: count, dtype: int64"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"filtered_train = train_df[lambda x: x[\"cleanText\"].str.contains(\"[A-Za-z0-9]\")]\nprint(filtered_train.shape)\nfiltered_dev = dev_df[lambda x: x[\"cleanText\"].str.contains(\"[A-Za-z0-9]\")]\nprint(filtered_dev.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:18.887648Z","iopub.execute_input":"2025-01-29T17:11:18.887929Z","iopub.status.idle":"2025-01-29T17:11:18.900643Z","shell.execute_reply.started":"2025-01-29T17:11:18.887910Z","shell.execute_reply":"2025-01-29T17:11:18.899630Z"}},"outputs":[{"name":"stdout","text":"(0, 3)\n(0, 3)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"train_df['enc_label'] = train_df['Class'].replace({'Abusive':1, 'Non-Abusive':0})\ndev_df['enc_label'] = dev_df['Class'].replace({'Abusive':1, 'Non-Abusive':0})\ntest_df['enc_label'] = test_df['Class'].replace({'Abusive':1, 'Non-Abusive':0})\ntrain_df.tail(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:21.881799Z","iopub.execute_input":"2025-01-29T17:11:21.882078Z","iopub.status.idle":"2025-01-29T17:11:21.896211Z","shell.execute_reply.started":"2025-01-29T17:11:21.882058Z","shell.execute_reply":"2025-01-29T17:11:21.895466Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                                   Text        Class  \\\n2785  ரெண்டுமே அவனா இல்ல அவளா....ஒரே கன்ஃப்யூசன்.......      Abusive   \n2786  ராஜேஷ்வரி செம்மயா இருக்கு ரொம்ப குளோசப்ல பார்க...      Abusive   \n2787  எனக்கு தெறிச்சு கார்த்தி னு ஒருத்தன் நல்லா ஓத்...      Abusive   \n2788  நீங்கள் கவலை கொல்லாதீர்கள் சகோதரி. சின்மை எவ்ள...      Abusive   \n2789  30 lakhs views and 9k comments அட பாவிங்களா அவ...  Non-Abusive   \n\n                                              cleanText  enc_label  \n2785  ரெண்டுமே அவனா இல்ல அவளாஒரே கன்ஃப்யூசன் கஷ்டம் ...          1  \n2786  ராஜேஷ்வரி செம்மயா இருக்கு ரொம்ப குளோசப்ல பார்க...          1  \n2787  எனக்கு தெறிச்சு கார்த்தி னு ஒருத்தன் நல்லா ஓத்...          1  \n2788  நீங்கள் கவலை கொல்லாதீர்கள் சகோதரி சின்மை எவ்ளோ...          1  \n2789  ௩௦ லக்ஸ் விஏவ்ஸ் அந்த் ௯க் சோம்மேந்த்ஸ் அட பாவ...          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Class</th>\n      <th>cleanText</th>\n      <th>enc_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2785</th>\n      <td>ரெண்டுமே அவனா இல்ல அவளா....ஒரே கன்ஃப்யூசன்.......</td>\n      <td>Abusive</td>\n      <td>ரெண்டுமே அவனா இல்ல அவளாஒரே கன்ஃப்யூசன் கஷ்டம் ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2786</th>\n      <td>ராஜேஷ்வரி செம்மயா இருக்கு ரொம்ப குளோசப்ல பார்க...</td>\n      <td>Abusive</td>\n      <td>ராஜேஷ்வரி செம்மயா இருக்கு ரொம்ப குளோசப்ல பார்க...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2787</th>\n      <td>எனக்கு தெறிச்சு கார்த்தி னு ஒருத்தன் நல்லா ஓத்...</td>\n      <td>Abusive</td>\n      <td>எனக்கு தெறிச்சு கார்த்தி னு ஒருத்தன் நல்லா ஓத்...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2788</th>\n      <td>நீங்கள் கவலை கொல்லாதீர்கள் சகோதரி. சின்மை எவ்ள...</td>\n      <td>Abusive</td>\n      <td>நீங்கள் கவலை கொல்லாதீர்கள் சகோதரி சின்மை எவ்ளோ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2789</th>\n      <td>30 lakhs views and 9k comments அட பாவிங்களா அவ...</td>\n      <td>Non-Abusive</td>\n      <td>௩௦ லக்ஸ் விஏவ்ஸ் அந்த் ௯க் சோம்மேந்த்ஸ் அட பாவ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"test_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:24.681455Z","iopub.execute_input":"2025-01-29T17:11:24.681745Z","iopub.status.idle":"2025-01-29T17:11:24.690762Z","shell.execute_reply.started":"2025-01-29T17:11:24.681725Z","shell.execute_reply":"2025-01-29T17:11:24.689971Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"   id                                               Text        Class  \\\n0   1  இவ ஒரு மானெங்கெட்ட பொறுக்கி. ஒரே ஒரு routine ஒ...      Abusive   \n1   2  இப்டியே பேசிக்கிட்டே இருந்தா எப்டி..... யாரு ப...  Non-Abusive   \n2   3  அடக் கடவுளே இது என்னக் கொடுமையை ஊருல உலகத்துல ...  Non-Abusive   \n3   4  இதற்கு ஒரு தீர்வு இருக்கு.  அவன் அவன் வேலை அவன...  Non-Abusive   \n4   5  தம்பி போய் நல்லவங்களை பேட்டிஎடு அவ சொல்வது அத்...      Abusive   \n5   6  சின்மயி வைரமுத்துவை பத்து வருடங்களுக்கு முன்பு...      Abusive   \n6   7                     இவ ஒரு அளவு தான் சப்பி இருக்கா      Abusive   \n7   8  உன்னை போட்டோ எடுத்து கக்கூஸ் ல தான் மாட்டி வைக...      Abusive   \n8   9  வாழ்க்கை மில் ஒவ்வொருவருக்கும் ஆயிரம் பிரச்சின...  Non-Abusive   \n9  10   இந்த ஐட்டம் சுகந்தி அவ்வளவு நல்லவ கிடையாது;!!!!!      Abusive   \n\n                                           cleanText  enc_label  \n0  இவ ஒரு மானெங்கெட்ட பொறுக்கி ஒரே ஒரு ரோஉதிநே ஒர...          1  \n1  இப்டியே பேசிக்கிட்டே இருந்தா எப்டி யாரு பெருசு...          0  \n2  அடக் கடவுளே இது என்னக் கொடுமையை ஊருல உலகத்துல ...          0  \n3  இதற்கு ஒரு தீர்வு இருக்கு அவன் அவன் வேலை அவன் ...          0  \n4  தம்பி போய் நல்லவங்களை பேட்டிஎடு அவ சொல்வது அத்...          1  \n5  சின்மயி வைரமுத்துவை பத்து வருடங்களுக்கு முன்பு...          1  \n6                     இவ ஒரு அளவு தான் சப்பி இருக்கா          1  \n7  உன்னை போட்டோ எடுத்து கக்கூஸ் ல தான் மாட்டி வைக...          1  \n8  வாழ்க்கை மில் ஒவ்வொருவருக்கும் ஆயிரம் பிரச்சின...          0  \n9         இந்த ஐட்டம் சுகந்தி அவ்வளவு நல்லவ கிடையாது          1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Text</th>\n      <th>Class</th>\n      <th>cleanText</th>\n      <th>enc_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>இவ ஒரு மானெங்கெட்ட பொறுக்கி. ஒரே ஒரு routine ஒ...</td>\n      <td>Abusive</td>\n      <td>இவ ஒரு மானெங்கெட்ட பொறுக்கி ஒரே ஒரு ரோஉதிநே ஒர...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>இப்டியே பேசிக்கிட்டே இருந்தா எப்டி..... யாரு ப...</td>\n      <td>Non-Abusive</td>\n      <td>இப்டியே பேசிக்கிட்டே இருந்தா எப்டி யாரு பெருசு...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>அடக் கடவுளே இது என்னக் கொடுமையை ஊருல உலகத்துல ...</td>\n      <td>Non-Abusive</td>\n      <td>அடக் கடவுளே இது என்னக் கொடுமையை ஊருல உலகத்துல ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>இதற்கு ஒரு தீர்வு இருக்கு.  அவன் அவன் வேலை அவன...</td>\n      <td>Non-Abusive</td>\n      <td>இதற்கு ஒரு தீர்வு இருக்கு அவன் அவன் வேலை அவன் ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>தம்பி போய் நல்லவங்களை பேட்டிஎடு அவ சொல்வது அத்...</td>\n      <td>Abusive</td>\n      <td>தம்பி போய் நல்லவங்களை பேட்டிஎடு அவ சொல்வது அத்...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>சின்மயி வைரமுத்துவை பத்து வருடங்களுக்கு முன்பு...</td>\n      <td>Abusive</td>\n      <td>சின்மயி வைரமுத்துவை பத்து வருடங்களுக்கு முன்பு...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>இவ ஒரு அளவு தான் சப்பி இருக்கா</td>\n      <td>Abusive</td>\n      <td>இவ ஒரு அளவு தான் சப்பி இருக்கா</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>உன்னை போட்டோ எடுத்து கக்கூஸ் ல தான் மாட்டி வைக...</td>\n      <td>Abusive</td>\n      <td>உன்னை போட்டோ எடுத்து கக்கூஸ் ல தான் மாட்டி வைக...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>வாழ்க்கை மில் ஒவ்வொருவருக்கும் ஆயிரம் பிரச்சின...</td>\n      <td>Non-Abusive</td>\n      <td>வாழ்க்கை மில் ஒவ்வொருவருக்கும் ஆயிரம் பிரச்சின...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>இந்த ஐட்டம் சுகந்தி அவ்வளவு நல்லவ கிடையாது;!!!!!</td>\n      <td>Abusive</td>\n      <td>இந்த ஐட்டம் சுகந்தி அவ்வளவு நல்லவ கிடையாது</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"train_df.enc_label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:27.959729Z","iopub.execute_input":"2025-01-29T17:11:27.960059Z","iopub.status.idle":"2025-01-29T17:11:27.967305Z","shell.execute_reply.started":"2025-01-29T17:11:27.960033Z","shell.execute_reply":"2025-01-29T17:11:27.966445Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"enc_label\n0    1421\n1    1358\nName: count, dtype: int64"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"dev_df.enc_label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:30.486991Z","iopub.execute_input":"2025-01-29T17:11:30.487282Z","iopub.status.idle":"2025-01-29T17:11:30.493601Z","shell.execute_reply.started":"2025-01-29T17:11:30.487261Z","shell.execute_reply":"2025-01-29T17:11:30.492859Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"enc_label\n0    320\n1    278\nName: count, dtype: int64"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_df.enc_label.value_counts()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:32.350742Z","iopub.execute_input":"2025-01-29T17:11:32.351123Z","iopub.status.idle":"2025-01-29T17:11:32.358963Z","shell.execute_reply.started":"2025-01-29T17:11:32.351092Z","shell.execute_reply":"2025-01-29T17:11:32.358125Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"enc_label\n0    1421\n1    1358\nName: count, dtype: int64"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"**ML_Models (LR,DT,RF,SVM)**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n\n# Prepare features and labels\nX_train, y_train = train_df['cleanText'], train_df['enc_label']\nX_dev, y_dev = dev_df['cleanText'], dev_df['enc_label']\nX_test, y_test = test_df['cleanText'], test_df['enc_label']\n\n# Combine training and development sets for better model generalization\nX_combined = pd.concat([X_train, X_dev], ignore_index=True)\ny_combined = pd.concat([y_train, y_dev], ignore_index=True)\n\n# TF-IDF Vectorization\ntfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), stop_words='english')\nX_train_tfidf = tfidf.fit_transform(X_combined)\nX_test_tfidf = tfidf.transform(X_test)\n\n# Initialize models\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'Support Vector Machine': SVC(probability=True)\n}\n\n# Train and evaluate each model\nfor model_name, model in models.items():\n    print(f\"\\nTraining {model_name}...\")\n    model.fit(X_train_tfidf, y_combined)\n    \n    # Predictions\n    y_pred = model.predict(X_test_tfidf)\n    \n    # Evaluation\n    precision = precision_score(y_test, y_pred, average='macro')\n    recall = recall_score(y_test, y_pred, average='macro')\n    f1 = f1_score(y_test, y_pred, average='macro')\n\n    print(f\"Results for {model_name}:\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"Macro F1 Score: {f1:.4f}\")\n\n# Hyperparameter tuning example for SVM using GridSearchCV\nparam_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\ngrid_search = GridSearchCV(SVC(), param_grid, cv=3, scoring='accuracy')\ngrid_search.fit(X_train_tfidf, y_combined)\nprint(\"\\nBest parameters for SVM:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T07:40:12.162999Z","iopub.execute_input":"2025-01-29T07:40:12.163336Z","iopub.status.idle":"2025-01-29T07:40:42.516816Z","shell.execute_reply.started":"2025-01-29T07:40:12.163308Z","shell.execute_reply":"2025-01-29T07:40:42.515870Z"}},"outputs":[{"name":"stdout","text":"\nTraining Logistic Regression...\nResults for Logistic Regression:\nPrecision: 0.7047\nRecall: 0.7045\nMacro F1 Score: 0.7040\n\nTraining Decision Tree...\nResults for Decision Tree:\nPrecision: 0.6085\nRecall: 0.6085\nMacro F1 Score: 0.6085\n\nTraining Random Forest...\nResults for Random Forest:\nPrecision: 0.6658\nRecall: 0.6635\nMacro F1 Score: 0.6614\n\nTraining Support Vector Machine...\nResults for Support Vector Machine:\nPrecision: 0.6895\nRecall: 0.6894\nMacro F1 Score: 0.6889\n\nBest parameters for SVM: {'C': 1, 'kernel': 'rbf'}\nBest score: 0.6867024537859351\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"**DL_Models (BiLSTM,CNN + LSTM)**","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Dropout, Flatten\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Parameters\nmax_len = 100\nembedding_dim = 128\n\n# Tokenization and Word2Vec Training\ntokenized_sentences = [sentence.split() for sentence in X_combined]\nword2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=embedding_dim, window=5, min_count=1)\nvocab_size = len(word2vec_model.wv.index_to_key)\n\n# Create Embedding Matrix\nembedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\nword_index = {word: idx for idx, word in enumerate(word2vec_model.wv.index_to_key, 1)}\n\nfor word, idx in word_index.items():\n    if word in word2vec_model.wv:\n        embedding_matrix[idx] = word2vec_model.wv[word]\n\n# Tokenizer Setup for Sequences\ntokenizer = Tokenizer()\ntokenizer.word_index = word_index\n\nX_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_combined), maxlen=max_len, padding='post')\nX_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len, padding='post')\n\n# Build BiLSTM Model\nbilstm_model = Sequential([\n    Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n    Bidirectional(LSTM(64, return_sequences=False)),\n    Dropout(0.5),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nbilstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(\"\\nTraining BiLSTM Model...\")\nbilstm_model.fit(X_train_seq, y_combined, epochs=5, batch_size=32, validation_split=0.2)\n\n# Evaluate BiLSTM\nprint(\"\\nEvaluating BiLSTM Model...\")\ny_pred_bilstm = (bilstm_model.predict(X_test_seq) > 0.5).astype(int)\n\nprecision_bilstm = precision_score(y_test, y_pred_bilstm, average='macro')\nrecall_bilstm = recall_score(y_test, y_pred_bilstm, average='macro')\nf1_bilstm = f1_score(y_test, y_pred_bilstm, average='macro')\n\nprint(f\"BiLSTM Results:\")\nprint(f\"Precision: {precision_bilstm:.4f}\")\nprint(f\"Recall: {recall_bilstm:.4f}\")\nprint(f\"Macro F1 Score: {f1_bilstm:.4f}\")\n\n# Build CNN + LSTM Model\ncnn_lstm_model = Sequential([\n    Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n    Conv1D(128, 5, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    LSTM(64, return_sequences=False),\n    Dropout(0.5),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\ncnn_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(\"\\nTraining CNN + LSTM Model...\")\ncnn_lstm_model.fit(X_train_seq, y_combined, epochs=5, batch_size=32, validation_split=0.2)\n\n# Evaluate CNN + LSTM\nprint(\"\\nEvaluating CNN + LSTM Model...\")\ny_pred_cnn_lstm = (cnn_lstm_model.predict(X_test_seq) > 0.5).astype(int)\n\nprecision_cnn_lstm = precision_score(y_test, y_pred_cnn_lstm, average='macro')\nrecall_cnn_lstm = recall_score(y_test, y_pred_cnn_lstm, average='macro')\nf1_cnn_lstm = f1_score(y_test, y_pred_cnn_lstm, average='macro')\n\nprint(f\"CNN + LSTM Results:\")\nprint(f\"Precision: {precision_cnn_lstm:.4f}\")\nprint(f\"Recall: {recall_cnn_lstm:.4f}\")\nprint(f\"Macro F1 Score: {f1_cnn_lstm:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T07:44:45.352129Z","iopub.execute_input":"2025-01-29T07:44:45.352762Z","iopub.status.idle":"2025-01-29T07:44:58.957895Z","shell.execute_reply.started":"2025-01-29T07:44:45.352736Z","shell.execute_reply":"2025-01-29T07:44:58.957174Z"}},"outputs":[{"name":"stdout","text":"\nTraining BiLSTM Model...\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5074 - loss: 0.6934 - val_accuracy: 0.5355 - val_loss: 0.6921\nEpoch 2/5\n\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5152 - loss: 0.6928 - val_accuracy: 0.5325 - val_loss: 0.6919\nEpoch 3/5\n\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5415 - loss: 0.6915 - val_accuracy: 0.4675 - val_loss: 0.6953\nEpoch 4/5\n\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5136 - loss: 0.6931 - val_accuracy: 0.5118 - val_loss: 0.6925\nEpoch 5/5\n\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4925 - loss: 0.6924 - val_accuracy: 0.5577 - val_loss: 0.6889\n\nEvaluating BiLSTM Model...\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\nBiLSTM Results:\nPrecision: 0.5562\nRecall: 0.5258\nMacro F1 Score: 0.4477\n\nTraining CNN + LSTM Model...\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.4877 - loss: 0.6932 - val_accuracy: 0.5370 - val_loss: 0.6925\nEpoch 2/5\n\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5077 - loss: 0.6928 - val_accuracy: 0.4675 - val_loss: 0.6926\nEpoch 3/5\n\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5104 - loss: 0.6937 - val_accuracy: 0.4675 - val_loss: 0.6924\nEpoch 4/5\n\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4902 - loss: 0.6926 - val_accuracy: 0.5355 - val_loss: 0.6912\nEpoch 5/5\n\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5133 - loss: 0.6907 - val_accuracy: 0.5355 - val_loss: 0.6909\n\nEvaluating CNN + LSTM Model...\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\nCNN + LSTM Results:\nPrecision: 0.2446\nRecall: 0.4983\nMacro F1 Score: 0.3281\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"**Transformers**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:42.392829Z","iopub.execute_input":"2025-01-29T17:11:42.393149Z","iopub.status.idle":"2025-01-29T17:11:43.039292Z","shell.execute_reply.started":"2025-01-29T17:11:42.393121Z","shell.execute_reply":"2025-01-29T17:11:43.038413Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"**mBERT**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for mBERT\nmodel_name = 'bert-base-multilingual-cased'  # Suitable for multilingual datasets like Tamil\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES\n)\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)  # Compute the loss here\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:18:55.897470Z","iopub.execute_input":"2025-01-29T17:18:55.897814Z","iopub.status.idle":"2025-01-29T17:23:18.417475Z","shell.execute_reply.started":"2025-01-29T17:18:55.897793Z","shell.execute_reply":"2025-01-29T17:23:18.416341Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"becf23bd3398469ead603f710c6ec4c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d827212ab19e4d0ebdafdb363a1c2899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d4dd903ff4436dbeb75c068a56dd78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c6af88168284304a0f335992e4cecb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56215c7ab1fa47c68848bd22dd3ec17b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|██████████| 87/87 [00:19<00:00,  4.51it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 18.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6722 | Train Accuracy: 0.5599\nValidation Loss: 0.5661 | Validation Accuracy: 0.6973 | F1 Score: 0.6952\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:18<00:00,  4.69it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 18.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.5434 | Train Accuracy: 0.7222\nValidation Loss: 0.5364 | Validation Accuracy: 0.7341 | F1 Score: 0.7243\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:18<00:00,  4.63it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 18.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.4465 | Train Accuracy: 0.7927\nValidation Loss: 0.4956 | Validation Accuracy: 0.7458 | F1 Score: 0.7386\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:19<00:00,  4.52it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 17.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.3576 | Train Accuracy: 0.8528\nValidation Loss: 0.4642 | Validation Accuracy: 0.7893 | F1 Score: 0.7896\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:19<00:00,  4.48it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 16.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.2701 | Train Accuracy: 0.8910\nValidation Loss: 0.6343 | Validation Accuracy: 0.7726 | F1 Score: 0.7717\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:19<00:00,  4.39it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 16.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.2218 | Train Accuracy: 0.9129\nValidation Loss: 0.6000 | Validation Accuracy: 0.7860 | F1 Score: 0.7860\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:20<00:00,  4.30it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 16.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.1525 | Train Accuracy: 0.9435\nValidation Loss: 0.6212 | Validation Accuracy: 0.7876 | F1 Score: 0.7878\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:20<00:00,  4.20it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.0877 | Train Accuracy: 0.9748\nValidation Loss: 0.6826 | Validation Accuracy: 0.7826 | F1 Score: 0.7829\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:21<00:00,  4.06it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 14.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.0753 | Train Accuracy: 0.9752\nValidation Loss: 0.7049 | Validation Accuracy: 0.7910 | F1 Score: 0.7911\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:21<00:00,  4.04it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.0619 | Train Accuracy: 0.9831\nValidation Loss: 0.7449 | Validation Accuracy: 0.7793 | F1 Score: 0.7795\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:21<00:00,  4.11it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.0592 | Train Accuracy: 0.9824\nValidation Loss: 0.7419 | Validation Accuracy: 0.7809 | F1 Score: 0.7812\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:21<00:00,  4.11it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.0508 | Train Accuracy: 0.9856\nValidation Loss: 0.7455 | Validation Accuracy: 0.7809 | F1 Score: 0.7812\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n# Print the Macro F1 Score ,Test Accuracy,Precision and Recall\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:23:37.678592Z","iopub.execute_input":"2025-01-29T17:23:37.679268Z","iopub.status.idle":"2025-01-29T17:23:42.250672Z","shell.execute_reply.started":"2025-01-29T17:23:37.679236Z","shell.execute_reply":"2025-01-29T17:23:42.249930Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|██████████| 19/19 [00:04<00:00,  4.17it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7657\nTest Accuracy: 0.7659\nPrecision: 0.7658\nRecall: 0.7657\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"**Hyperparameter Tuning**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for mBERT\nmodel_name = 'bert-base-multilingual-cased'  # Suitable for multilingual datasets like Tamil\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 16\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES\n)\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)  # Compute the loss here\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 8\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:23:53.715612Z","iopub.execute_input":"2025-01-29T17:23:53.715921Z","iopub.status.idle":"2025-01-29T17:27:46.447730Z","shell.execute_reply.started":"2025-01-29T17:23:53.715899Z","shell.execute_reply":"2025-01-29T17:27:46.446844Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|██████████| 174/174 [00:27<00:00,  6.28it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 27.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/8\nTrain Loss: 0.6502 | Train Accuracy: 0.6017\nValidation Loss: 0.6007 | Validation Accuracy: 0.6555 | F1 Score: 0.6388\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:27<00:00,  6.22it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 28.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/8\nTrain Loss: 0.5345 | Train Accuracy: 0.7265\nValidation Loss: 0.5309 | Validation Accuracy: 0.7559 | F1 Score: 0.7528\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:27<00:00,  6.31it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 29.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/8\nTrain Loss: 0.4339 | Train Accuracy: 0.8078\nValidation Loss: 0.4807 | Validation Accuracy: 0.7843 | F1 Score: 0.7815\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:27<00:00,  6.30it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 28.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/8\nTrain Loss: 0.3395 | Train Accuracy: 0.8589\nValidation Loss: 0.5447 | Validation Accuracy: 0.7843 | F1 Score: 0.7845\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:27<00:00,  6.27it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 28.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/8\nTrain Loss: 0.2527 | Train Accuracy: 0.8992\nValidation Loss: 0.5335 | Validation Accuracy: 0.7709 | F1 Score: 0.7706\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:27<00:00,  6.30it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 28.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/8\nTrain Loss: 0.1638 | Train Accuracy: 0.9403\nValidation Loss: 0.7809 | Validation Accuracy: 0.7692 | F1 Score: 0.7694\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:27<00:00,  6.33it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 29.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/8\nTrain Loss: 0.0798 | Train Accuracy: 0.9719\nValidation Loss: 0.8181 | Validation Accuracy: 0.7726 | F1 Score: 0.7727\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:27<00:00,  6.34it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 29.16it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 8/8\nTrain Loss: 0.0547 | Train Accuracy: 0.9852\nValidation Loss: 0.8840 | Validation Accuracy: 0.7676 | F1 Score: 0.7678\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n# Print the Macro F1 Score ,Test Accuracy,Precision and Recall\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:27:50.571544Z","iopub.execute_input":"2025-01-29T17:27:50.571885Z","iopub.status.idle":"2025-01-29T17:27:55.664007Z","shell.execute_reply.started":"2025-01-29T17:27:50.571860Z","shell.execute_reply":"2025-01-29T17:27:55.663098Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|██████████| 38/38 [00:05<00:00,  7.49it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7686\nTest Accuracy: 0.7692\nPrecision: 0.7704\nRecall: 0.7685\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"**XLM-RoBERTa**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for XLM-RoBERTa\nmodel_name = 'xlm-roberta-base'  # Switching to XLM-RoBERTa\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES\n)\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)  # Compute the loss here\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:31:08.144003Z","iopub.execute_input":"2025-01-29T17:31:08.144393Z","iopub.status.idle":"2025-01-29T17:36:56.275509Z","shell.execute_reply.started":"2025-01-29T17:31:08.144363Z","shell.execute_reply":"2025-01-29T17:36:56.274494Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"542993e39e534e0c839c16be7202c1e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e2bda490d024e2da8dfedbc3e1a42a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b902b743bc4eb5a9291f07deda55ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccf2b1b372044c1ebc7cc6487cf705b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caa162ef2fbf400f8ee989675c9f3284"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|██████████| 87/87 [00:25<00:00,  3.38it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 14.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6903 | Train Accuracy: 0.5264\nValidation Loss: 0.6403 | Validation Accuracy: 0.6371 | F1 Score: 0.6241\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:26<00:00,  3.27it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.5866 | Train Accuracy: 0.6931\nValidation Loss: 0.4898 | Validation Accuracy: 0.7726 | F1 Score: 0.7723\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.16it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 12.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.5250 | Train Accuracy: 0.7499\nValidation Loss: 0.4735 | Validation Accuracy: 0.7692 | F1 Score: 0.7692\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:26<00:00,  3.23it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.4415 | Train Accuracy: 0.8021\nValidation Loss: 0.4675 | Validation Accuracy: 0.7910 | F1 Score: 0.7911\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:26<00:00,  3.24it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.3860 | Train Accuracy: 0.8377\nValidation Loss: 0.5131 | Validation Accuracy: 0.7793 | F1 Score: 0.7793\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.21it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.3436 | Train Accuracy: 0.8636\nValidation Loss: 0.5002 | Validation Accuracy: 0.7993 | F1 Score: 0.7987\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.21it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.2844 | Train Accuracy: 0.8942\nValidation Loss: 0.6060 | Validation Accuracy: 0.7793 | F1 Score: 0.7789\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:26<00:00,  3.22it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.2055 | Train Accuracy: 0.9259\nValidation Loss: 0.6172 | Validation Accuracy: 0.7759 | F1 Score: 0.7759\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:26<00:00,  3.22it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.1902 | Train Accuracy: 0.9367\nValidation Loss: 0.6415 | Validation Accuracy: 0.7843 | F1 Score: 0.7841\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:26<00:00,  3.22it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.1731 | Train Accuracy: 0.9395\nValidation Loss: 0.6655 | Validation Accuracy: 0.7826 | F1 Score: 0.7823\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.20it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.1709 | Train Accuracy: 0.9406\nValidation Loss: 0.6694 | Validation Accuracy: 0.7843 | F1 Score: 0.7839\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.20it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.37it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.1747 | Train Accuracy: 0.9431\nValidation Loss: 0.6682 | Validation Accuracy: 0.7793 | F1 Score: 0.7790\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n# Print the Macro F1 Score, Test Accuracy, and Confusion Matrix\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\n\n# Plotting the confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:37:49.348853Z","iopub.execute_input":"2025-01-29T17:37:49.349162Z","iopub.status.idle":"2025-01-29T17:37:53.827876Z","shell.execute_reply.started":"2025-01-29T17:37:49.349138Z","shell.execute_reply":"2025-01-29T17:37:53.826851Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|██████████| 19/19 [00:04<00:00,  4.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.8009\nTest Accuracy: 0.8010\nPrecision: 0.8026\nRecall: 0.8017\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"**Hyperparameter Tuning**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for XLM-RoBERTa\nmodel_name = 'xlm-roberta-base'  # Switching to XLM-RoBERTa\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 16\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES\n)\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)  # Compute the loss here\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 8\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:38:05.102895Z","iopub.execute_input":"2025-01-29T17:38:05.103249Z","iopub.status.idle":"2025-01-29T17:43:17.200235Z","shell.execute_reply.started":"2025-01-29T17:38:05.103210Z","shell.execute_reply":"2025-01-29T17:43:17.199318Z"}},"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|██████████| 174/174 [00:37<00:00,  4.69it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 24.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/8\nTrain Loss: 0.6960 | Train Accuracy: 0.5038\nValidation Loss: 0.6876 | Validation Accuracy: 0.5753 | F1 Score: 0.5319\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:37<00:00,  4.65it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 25.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/8\nTrain Loss: 0.6664 | Train Accuracy: 0.6078\nValidation Loss: 0.5980 | Validation Accuracy: 0.6572 | F1 Score: 0.6331\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:37<00:00,  4.69it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 25.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/8\nTrain Loss: 0.5896 | Train Accuracy: 0.7017\nValidation Loss: 0.5490 | Validation Accuracy: 0.7475 | F1 Score: 0.7478\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:37<00:00,  4.65it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 25.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/8\nTrain Loss: 0.5367 | Train Accuracy: 0.7431\nValidation Loss: 0.6180 | Validation Accuracy: 0.7207 | F1 Score: 0.7061\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:37<00:00,  4.68it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 25.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/8\nTrain Loss: 0.4762 | Train Accuracy: 0.7852\nValidation Loss: 0.5272 | Validation Accuracy: 0.7726 | F1 Score: 0.7718\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:36<00:00,  4.70it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 25.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/8\nTrain Loss: 0.4172 | Train Accuracy: 0.8176\nValidation Loss: 0.5160 | Validation Accuracy: 0.7876 | F1 Score: 0.7879\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:37<00:00,  4.69it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 25.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/8\nTrain Loss: 0.3689 | Train Accuracy: 0.8435\nValidation Loss: 0.5760 | Validation Accuracy: 0.7726 | F1 Score: 0.7725\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 174/174 [00:37<00:00,  4.68it/s]\nValidation Batches: 100%|██████████| 38/38 [00:01<00:00, 25.01it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 8/8\nTrain Loss: 0.3227 | Train Accuracy: 0.8687\nValidation Loss: 0.5260 | Validation Accuracy: 0.8077 | F1 Score: 0.8070\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n# Print the Macro F1 Score, Test Accuracy, and Confusion Matrix\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\n\n# Plotting the confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:43:52.036815Z","iopub.execute_input":"2025-01-29T17:43:52.037150Z","iopub.status.idle":"2025-01-29T17:43:56.808748Z","shell.execute_reply.started":"2025-01-29T17:43:52.037125Z","shell.execute_reply":"2025-01-29T17:43:56.807669Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|██████████| 38/38 [00:04<00:00,  8.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7773\nTest Accuracy: 0.7776\nPrecision: 0.7808\nRecall: 0.7786\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Store sample texts, true labels, and predictions\nsample_texts = []  \nall_preds = []\nall_targets = []\n\n# Iterate over the test set to collect predictions\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(test_loader):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, 1)\n\n        # Store predictions and actual labels\n        all_preds.extend(predicted.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n\n        # Collect the 'cleanText' for the current batch from the test dataframe\n        start_idx = batch_idx * BATCH_SIZE\n        end_idx = start_idx + len(batch['ids'])\n        sample_texts.extend(test_df['cleanText'].iloc[start_idx:end_idx].tolist())\n\n# Randomly select some samples to display\nnum_samples = 5  # Number of examples to display\nsample_indices = np.random.choice(len(all_targets), num_samples, replace=False)\n\n# Prepare the sample data for display\nsample_data = {\n    \"Text\": [sample_texts[i] for i in sample_indices],\n    \"Actual Label and Predicted Label\": [f\"{all_targets[i]} -> {all_preds[i]}\" for i in sample_indices]\n}\n\n# Create a DataFrame\nsample_df = pd.DataFrame(sample_data)\n\n# Display the DataFrame with the format \"Actual Label -> Predicted Label\"\nprint(\"\\nSample Predictions:\")\nprint(sample_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T10:32:53.662707Z","iopub.execute_input":"2025-01-29T10:32:53.663053Z","iopub.status.idle":"2025-01-29T10:32:57.411812Z","shell.execute_reply.started":"2025-01-29T10:32:53.663025Z","shell.execute_reply":"2025-01-29T10:32:57.410853Z"}},"outputs":[{"name":"stdout","text":"\nSample Predictions:\n                                                Text  \\\n0  சச்ச திவ்யா முன் டா மே உண்மையில் அந்தப் பரட்டை...   \n1  உன்னோட ஆசைக்காக பெத்தவங்கள கஷ்ட படுத்தாத நீதான...   \n2  பெரிய ஹீரோயின் கூட இப்பிடி ஸேல்ஃபிஏ ஏதுக்க மத்...   \n3  உங்க பாலொவர்சை பார்க்கும் போது தெரியுது ஷகிலாவ...   \n4  இப்படி ஒரு பெண் இருக்குறதுக்கு இல்லாம இருக்கலா...   \n\n  Actual Label and Predicted Label  \n0                           1 -> 0  \n1                           1 -> 1  \n2                           0 -> 0  \n3                           1 -> 1  \n4                           1 -> 1  \n","output_type":"stream"}],"execution_count":152},{"cell_type":"markdown","source":"**Indic Sentence-BERT**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer and model for Indic Sentence-BERT\nmodel_name = 'l3cube-pune/indic-sentence-bert-nli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Set `num_labels` for binary classification\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Loss and optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Mixed-precision training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:44:11.575108Z","iopub.execute_input":"2025-01-29T17:44:11.575459Z","iopub.status.idle":"2025-01-29T17:49:10.374373Z","shell.execute_reply.started":"2025-01-29T17:44:11.575432Z","shell.execute_reply":"2025-01-29T17:49:10.373530Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"063b87d1609a44ec8309dd34332378d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b73a8e2cb9947c9a753f361ffde89ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98278f52987642b48a13ec2e66b9af92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ebc08a6e8b454089c14219571449e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/668 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fc5d5e8a8584b0a93feb1a88dcec3a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/950M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb453f95ef748a7b852ec1952d62f17"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at l3cube-pune/indic-sentence-bert-nli and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|██████████| 87/87 [00:22<00:00,  3.86it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6860 | Train Accuracy: 0.6402\nValidation Loss: 0.6658 | Validation Accuracy: 0.6839 | F1 Score: 0.6629\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:23<00:00,  3.69it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 14.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.6150 | Train Accuracy: 0.7600\nValidation Loss: 0.5654 | Validation Accuracy: 0.7642 | F1 Score: 0.7645\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:23<00:00,  3.73it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.5138 | Train Accuracy: 0.8165\nValidation Loss: 0.5199 | Validation Accuracy: 0.7676 | F1 Score: 0.7670\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.4267 | Train Accuracy: 0.8510\nValidation Loss: 0.4895 | Validation Accuracy: 0.7826 | F1 Score: 0.7817\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:23<00:00,  3.78it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.3502 | Train Accuracy: 0.8856\nValidation Loss: 0.4735 | Validation Accuracy: 0.8027 | F1 Score: 0.8027\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:23<00:00,  3.75it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.2947 | Train Accuracy: 0.9064\nValidation Loss: 0.5124 | Validation Accuracy: 0.7876 | F1 Score: 0.7872\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.2548 | Train Accuracy: 0.9219\nValidation Loss: 0.5091 | Validation Accuracy: 0.8027 | F1 Score: 0.8027\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.2097 | Train Accuracy: 0.9410\nValidation Loss: 0.5190 | Validation Accuracy: 0.8194 | F1 Score: 0.8188\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:23<00:00,  3.78it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.1804 | Train Accuracy: 0.9514\nValidation Loss: 0.5690 | Validation Accuracy: 0.7843 | F1 Score: 0.7842\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.78it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.1654 | Train Accuracy: 0.9532\nValidation Loss: 0.5501 | Validation Accuracy: 0.8094 | F1 Score: 0.8095\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:23<00:00,  3.77it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.1401 | Train Accuracy: 0.9593\nValidation Loss: 0.5789 | Validation Accuracy: 0.7960 | F1 Score: 0.7962\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:23<00:00,  3.75it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.38it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.1131 | Train Accuracy: 0.9701\nValidation Loss: 0.5865 | Validation Accuracy: 0.8027 | F1 Score: 0.8029\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\n\n# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n# Print the Macro F1 Score and Test Accuracy\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:49:36.998025Z","iopub.execute_input":"2025-01-29T17:49:36.998366Z","iopub.status.idle":"2025-01-29T17:49:41.432850Z","shell.execute_reply.started":"2025-01-29T17:49:36.998323Z","shell.execute_reply":"2025-01-29T17:49:41.431892Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|██████████| 19/19 [00:04<00:00,  4.30it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7684\nTest Accuracy: 0.7692\nPrecision: 0.7710\nRecall: 0.7683\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"**DistilBERT-mc**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer and model for distilbert-base-multilingual-cased\nmodel_name = 'distilbert-base-multilingual-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Set `num_labels` for binary classification\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Loss and optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Mixed-precision training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:49:53.380935Z","iopub.execute_input":"2025-01-29T17:49:53.381438Z","iopub.status.idle":"2025-01-29T17:52:47.093167Z","shell.execute_reply.started":"2025-01-29T17:49:53.381383Z","shell.execute_reply":"2025-01-29T17:52:47.092378Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49d6f15f30884e4cb32611b354170e2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c47c4c5b092140ada6be56cd9fe41ab6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53cd0f8f4c384878aa9bf6daaed469a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"894bd438bef4478aa98e73bb50cc40cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8eab3769e9a443b9fecc7ce46332858"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|██████████| 87/87 [00:13<00:00,  6.60it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 25.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6790 | Train Accuracy: 0.5603\nValidation Loss: 0.6099 | Validation Accuracy: 0.6555 | F1 Score: 0.6556\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 24.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5612 | Train Accuracy: 0.7107\nValidation Loss: 0.5168 | Validation Accuracy: 0.7559 | F1 Score: 0.7561\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.44it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 24.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5059 | Train Accuracy: 0.7503\nValidation Loss: 0.5284 | Validation Accuracy: 0.7375 | F1 Score: 0.7340\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.41it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 25.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4430 | Train Accuracy: 0.7978\nValidation Loss: 0.4906 | Validation Accuracy: 0.7575 | F1 Score: 0.7578\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.53it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 25.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3845 | Train Accuracy: 0.8356\nValidation Loss: 0.5364 | Validation Accuracy: 0.7542 | F1 Score: 0.7513\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.60it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 25.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3361 | Train Accuracy: 0.8611\nValidation Loss: 0.5196 | Validation Accuracy: 0.7575 | F1 Score: 0.7577\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.59it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 25.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2782 | Train Accuracy: 0.8852\nValidation Loss: 0.6017 | Validation Accuracy: 0.7525 | F1 Score: 0.7526\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.58it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 25.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2216 | Train Accuracy: 0.9104\nValidation Loss: 0.6728 | Validation Accuracy: 0.7391 | F1 Score: 0.7350\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 24.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1972 | Train Accuracy: 0.9205\nValidation Loss: 0.6785 | Validation Accuracy: 0.7341 | F1 Score: 0.7341\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.54it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 25.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1297 | Train Accuracy: 0.9550\nValidation Loss: 0.7890 | Validation Accuracy: 0.7441 | F1 Score: 0.7444\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.55it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 24.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1021 | Train Accuracy: 0.9644\nValidation Loss: 0.8548 | Validation Accuracy: 0.7542 | F1 Score: 0.7539\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:13<00:00,  6.59it/s]\nValidation Batches: 100%|██████████| 19/19 [00:00<00:00, 25.12it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0845 | Train Accuracy: 0.9673\nValidation Loss: 0.9205 | Validation Accuracy: 0.7408 | F1 Score: 0.7411\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\n\n# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the Macro F1 Score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the Test Accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n\n# Print the results\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:56:31.821597Z","iopub.execute_input":"2025-01-29T17:56:31.821947Z","iopub.status.idle":"2025-01-29T17:56:34.071749Z","shell.execute_reply.started":"2025-01-29T17:56:31.821925Z","shell.execute_reply":"2025-01-29T17:56:34.070853Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|██████████| 19/19 [00:02<00:00,  8.51it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7485\nTest Accuracy: 0.7492\nPrecision: 0.7500\nRecall: 0.7485\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"**DeOffXLMR-Tamil**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer and model name\nmodel_name = 'Hate-speech-CNERG/deoffxlmr-mono-tamil'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets (replace train_df and dev_df with your actual data)\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES ,ignore_mismatched_sizes=True\n)\n\n# Resize the classification head to match NUM_CLASSES\nmodel.classifier.out_proj = torch.nn.Linear(model.config.hidden_size, NUM_CLASSES)\n\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:56:39.693757Z","iopub.execute_input":"2025-01-29T17:56:39.694090Z","iopub.status.idle":"2025-01-29T18:02:40.118518Z","shell.execute_reply.started":"2025-01-29T17:56:39.694060Z","shell.execute_reply":"2025-01-29T18:02:40.117697Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/211 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58da4cf639034f9692a396bab90d376a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3689240ed560426db6cfc426fd7b678a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ff7c66f8fe340ca96716725b90f3f79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51594808dd27400f90db1848e8c762d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a3fcb5c8f154bfb83c0afc090361a1c"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Hate-speech-CNERG/deoffxlmr-mono-tamil and are newly initialized because the shapes did not match:\n- classifier.out_proj.weight: found shape torch.Size([6, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n- classifier.out_proj.bias: found shape torch.Size([6]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|██████████| 87/87 [00:25<00:00,  3.38it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 14.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6288 | Train Accuracy: 0.6387\nValidation Loss: 0.5536 | Validation Accuracy: 0.7358 | F1 Score: 0.7314\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:26<00:00,  3.28it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.4935 | Train Accuracy: 0.7686\nValidation Loss: 0.5054 | Validation Accuracy: 0.7559 | F1 Score: 0.7560\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.17it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 12.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.3892 | Train Accuracy: 0.8381\nValidation Loss: 0.5108 | Validation Accuracy: 0.7759 | F1 Score: 0.7759\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.20it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.3167 | Train Accuracy: 0.8723\nValidation Loss: 0.5238 | Validation Accuracy: 0.7692 | F1 Score: 0.7694\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:26<00:00,  3.24it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.2256 | Train Accuracy: 0.9162\nValidation Loss: 0.6295 | Validation Accuracy: 0.7676 | F1 Score: 0.7651\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.20it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.1509 | Train Accuracy: 0.9539\nValidation Loss: 0.6488 | Validation Accuracy: 0.7709 | F1 Score: 0.7709\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.22it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.1405 | Train Accuracy: 0.9547\nValidation Loss: 0.6599 | Validation Accuracy: 0.7793 | F1 Score: 0.7791\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.22it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.1256 | Train Accuracy: 0.9590\nValidation Loss: 0.6828 | Validation Accuracy: 0.7776 | F1 Score: 0.7777\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.22it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.1164 | Train Accuracy: 0.9601\nValidation Loss: 0.6835 | Validation Accuracy: 0.7759 | F1 Score: 0.7759\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.21it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.1132 | Train Accuracy: 0.9647\nValidation Loss: 0.6862 | Validation Accuracy: 0.7776 | F1 Score: 0.7776\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.21it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.1104 | Train Accuracy: 0.9651\nValidation Loss: 0.6883 | Validation Accuracy: 0.7759 | F1 Score: 0.7759\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:27<00:00,  3.22it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.51it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.1165 | Train Accuracy: 0.9622\nValidation Loss: 0.6885 | Validation Accuracy: 0.7759 | F1 Score: 0.7759\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the Macro F1 Score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the Test Accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n\n# Print the results\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:02:44.362233Z","iopub.execute_input":"2025-01-29T18:02:44.362554Z","iopub.status.idle":"2025-01-29T18:02:48.745645Z","shell.execute_reply.started":"2025-01-29T18:02:44.362531Z","shell.execute_reply":"2025-01-29T18:02:48.744773Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|██████████| 19/19 [00:04<00:00,  4.35it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7675\nTest Accuracy: 0.7676\nPrecision: 0.7675\nRecall: 0.7676\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"**Tamil BERT**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for Tamil BERT\nmodel_name = 'ai4bharat/indic-bert'  # Tamil-specific pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\nNUM_CLASSES = 2  # Update based on your dataset (e.g., positive/negative sentiment)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES\n)\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)  # Compute the loss here\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:02:52.907044Z","iopub.execute_input":"2025-01-29T18:02:52.907337Z","iopub.status.idle":"2025-01-29T18:06:48.168843Z","shell.execute_reply.started":"2025-01-29T18:02:52.907316Z","shell.execute_reply":"2025-01-29T18:06:48.167946Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2f5927775e4185aea70cf6dd4e585e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0fc91e0e3f4d3fbfdc291df49f56ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b805d978ec64cc5bc59af2ec34164ae"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|██████████| 87/87 [00:17<00:00,  4.93it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6930 | Train Accuracy: 0.5121\nValidation Loss: 0.6925 | Validation Accuracy: 0.5351 | F1 Score: 0.3731\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:18<00:00,  4.74it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 12.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.6926 | Train Accuracy: 0.5257\nValidation Loss: 0.6924 | Validation Accuracy: 0.5468 | F1 Score: 0.4194\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:18<00:00,  4.82it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.6917 | Train Accuracy: 0.5621\nValidation Loss: 0.6914 | Validation Accuracy: 0.5803 | F1 Score: 0.5756\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:17<00:00,  4.96it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.6887 | Train Accuracy: 0.5952\nValidation Loss: 0.6890 | Validation Accuracy: 0.5719 | F1 Score: 0.5693\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:17<00:00,  4.96it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.6800 | Train Accuracy: 0.6438\nValidation Loss: 0.6819 | Validation Accuracy: 0.6020 | F1 Score: 0.6008\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:17<00:00,  4.90it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.6661 | Train Accuracy: 0.6808\nValidation Loss: 0.6757 | Validation Accuracy: 0.6037 | F1 Score: 0.6032\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:17<00:00,  4.86it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.6510 | Train Accuracy: 0.6920\nValidation Loss: 0.6669 | Validation Accuracy: 0.6438 | F1 Score: 0.6377\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:17<00:00,  4.92it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.6302 | Train Accuracy: 0.7154\nValidation Loss: 0.6609 | Validation Accuracy: 0.6555 | F1 Score: 0.6541\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:17<00:00,  4.94it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.6098 | Train Accuracy: 0.7409\nValidation Loss: 0.6549 | Validation Accuracy: 0.6555 | F1 Score: 0.6473\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:17<00:00,  4.94it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.5914 | Train Accuracy: 0.7517\nValidation Loss: 0.6828 | Validation Accuracy: 0.6421 | F1 Score: 0.6415\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:17<00:00,  4.91it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.5682 | Train Accuracy: 0.7744\nValidation Loss: 0.6589 | Validation Accuracy: 0.6589 | F1 Score: 0.6568\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:17<00:00,  4.90it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 13.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.5531 | Train Accuracy: 0.7845\nValidation Loss: 0.6819 | Validation Accuracy: 0.6555 | F1 Score: 0.6559\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the Macro F1 Score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the Test Accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n\n# Print the results\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:07:36.112134Z","iopub.execute_input":"2025-01-29T18:07:36.112461Z","iopub.status.idle":"2025-01-29T18:07:40.737109Z","shell.execute_reply.started":"2025-01-29T18:07:36.112438Z","shell.execute_reply":"2025-01-29T18:07:40.736175Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|██████████| 19/19 [00:04<00:00,  4.13it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.6789\nTest Accuracy: 0.6806\nPrecision: 0.6821\nRecall: 0.6794\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"**MuRIL**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer and model name\nmodel_name = 'google/muril-base-cased'  # Changed model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets (replace train_df and dev_df with your actual data)\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n)\n\n# Resize the classification head to match NUM_CLASSES\nmodel.classifier.out_proj = torch.nn.Linear(model.config.hidden_size, NUM_CLASSES)\n\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:07:45.107257Z","iopub.execute_input":"2025-01-29T18:07:45.107636Z","iopub.status.idle":"2025-01-29T18:12:43.774680Z","shell.execute_reply.started":"2025-01-29T18:07:45.107608Z","shell.execute_reply":"2025-01-29T18:12:43.773744Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fe67dcd13a949dd8d66fa8ee793f08f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88737626a2274caa9381cdf54f71d790"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d994672b818462ab438ae7583c3957e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a9d188f34746e4b252ba40b4bdf4a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfd55dd406334146902330956bd0a9ed"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|██████████| 87/87 [00:22<00:00,  3.84it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6921 | Train Accuracy: 0.5480\nValidation Loss: 0.6856 | Validation Accuracy: 0.6873 | F1 Score: 0.6823\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:23<00:00,  3.73it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.6481 | Train Accuracy: 0.7344\nValidation Loss: 0.6152 | Validation Accuracy: 0.7191 | F1 Score: 0.7168\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:23<00:00,  3.75it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.5633 | Train Accuracy: 0.7895\nValidation Loss: 0.5619 | Validation Accuracy: 0.7492 | F1 Score: 0.7450\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.82it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 16.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.4875 | Train Accuracy: 0.8215\nValidation Loss: 0.5186 | Validation Accuracy: 0.7625 | F1 Score: 0.7626\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.4086 | Train Accuracy: 0.8600\nValidation Loss: 0.5012 | Validation Accuracy: 0.7776 | F1 Score: 0.7779\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:23<00:00,  3.78it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.3342 | Train Accuracy: 0.8888\nValidation Loss: 0.5111 | Validation Accuracy: 0.7793 | F1 Score: 0.7792\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 16.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.3005 | Train Accuracy: 0.8956\nValidation Loss: 0.5515 | Validation Accuracy: 0.7692 | F1 Score: 0.7679\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.82it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 16.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.2335 | Train Accuracy: 0.9295\nValidation Loss: 0.6014 | Validation Accuracy: 0.7609 | F1 Score: 0.7601\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.82it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.1940 | Train Accuracy: 0.9485\nValidation Loss: 0.5977 | Validation Accuracy: 0.7592 | F1 Score: 0.7595\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.1786 | Train Accuracy: 0.9547\nValidation Loss: 0.6074 | Validation Accuracy: 0.7625 | F1 Score: 0.7628\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.79it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.1729 | Train Accuracy: 0.9565\nValidation Loss: 0.6188 | Validation Accuracy: 0.7609 | F1 Score: 0.7612\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|██████████| 87/87 [00:22<00:00,  3.79it/s]\nValidation Batches: 100%|██████████| 19/19 [00:01<00:00, 15.85it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.1640 | Train Accuracy: 0.9608\nValidation Loss: 0.6200 | Validation Accuracy: 0.7592 | F1 Score: 0.7595\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the test accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n\n# Print the results\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:13:25.400083Z","iopub.execute_input":"2025-01-29T18:13:25.400480Z","iopub.status.idle":"2025-01-29T18:13:29.820636Z","shell.execute_reply.started":"2025-01-29T18:13:25.400446Z","shell.execute_reply":"2025-01-29T18:13:29.819754Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|██████████| 19/19 [00:04<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7723\nTest Accuracy: 0.7726\nPrecision: 0.7727\nRecall: 0.7722\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":43}]}