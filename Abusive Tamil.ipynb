{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10503977,"sourceType":"datasetVersion","datasetId":6456866}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\nimport os\nfrom glob import glob\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport re\nimport nltk\nimport json\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\nfrom sklearn.metrics import average_precision_score,roc_auc_score, roc_curve, precision_recall_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nnp.random.seed(42)\nimport nltk\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import ToktokTokenizer\nimport nltk, string, re, spacy,unicodedata, random\nfrom bs4 import BeautifulSoup\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:09:50.792539Z","iopub.execute_input":"2025-01-29T17:09:50.792905Z","iopub.status.idle":"2025-01-29T17:09:50.802179Z","shell.execute_reply.started":"2025-01-29T17:09:50.792881Z","shell.execute_reply":"2025-01-29T17:09:50.801306Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nCPU times: user 1.16 ms, sys: 0 ns, total: 1.16 ms\nWall time: 1.11 ms\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/abusive/AWT_train.csv\")\ndev_df = pd.read_csv(\"/kaggle/input/abusive/AWT_dev.csv\")\ntest_df = pd.read_csv('/kaggle/input/abusive/AWT_test_with_labels.csv')\n\nprint(\"Total number of training samples:\", len(train_df))\nprint(\"Total number of dev samples:\", len(dev_df))\nprint(\"Total number of test samples:\", len(test_df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:09:53.906638Z","iopub.execute_input":"2025-01-29T17:09:53.906954Z","iopub.status.idle":"2025-01-29T17:09:53.996916Z","shell.execute_reply.started":"2025-01-29T17:09:53.906929Z","shell.execute_reply":"2025-01-29T17:09:53.996203Z"}},"outputs":[{"name":"stdout","text":"Total number of training samples: 2790\nTotal number of dev samples: 598\nTotal number of test samples: 598\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:09:56.341724Z","iopub.execute_input":"2025-01-29T17:09:56.341999Z","iopub.status.idle":"2025-01-29T17:09:56.357062Z","shell.execute_reply.started":"2025-01-29T17:09:56.341977Z","shell.execute_reply":"2025-01-29T17:09:56.356125Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                Text        Class\n0       роЗродро▓рпНро▓роорпН роТро░рпБ родрпАро░рпНрокрпНрокрпБ роиро╛роЯрпНроЯро╛роорпИ родрпАро░рпНрокрпНрокрпИ рооро▒рпНро▒рпБ  Non-Abusive\n1           ропро╛ро░рпБроЯро╛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐, роирпА роОроЩрпНроХроЯро╛ роЗро░рпБроХрпНроХ ?  Non-Abusive\n2  роЗро░рогрпНроЯрпБ рокрпЗро░ро┐ройрпН (рокрпБрогрпНроЯрпИропрпИ) роЪро╛рооро╛ройрпНроХро│рпИ роХрпЛрогро┐роп роКроЪро╛ро▓рпН...      Abusive\n3  роОройрпНрой родро┐рооро┐ро░рпН роЗроирпНрод рокрпКрогрпНрогрпБроХрпНроХрпБ.....роороХрпНроХро│рпЗ роЗройрпНройрпБроорпН...      Abusive\n4          роРропрпЛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐роХрпН роО роХро╛роЯрпНроЯрпБроЩрпНроХ рокро╛ please  Non-Abusive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>роЗродро▓рпНро▓роорпН роТро░рпБ родрпАро░рпНрокрпНрокрпБ роиро╛роЯрпНроЯро╛роорпИ родрпАро░рпНрокрпНрокрпИ рооро▒рпНро▒рпБ</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ропро╛ро░рпБроЯро╛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐, роирпА роОроЩрпНроХроЯро╛ роЗро░рпБроХрпНроХ ?</td>\n      <td>Non-Abusive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>роЗро░рогрпНроЯрпБ рокрпЗро░ро┐ройрпН (рокрпБрогрпНроЯрпИропрпИ) роЪро╛рооро╛ройрпНроХро│рпИ роХрпЛрогро┐роп роКроЪро╛ро▓рпН...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>роОройрпНрой родро┐рооро┐ро░рпН роЗроирпНрод рокрпКрогрпНрогрпБроХрпНроХрпБ.....роороХрпНроХро│рпЗ роЗройрпНройрпБроорпН...</td>\n      <td>Abusive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>роРропрпЛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐роХрпН роО роХро╛роЯрпНроЯрпБроЩрпНроХ рокро╛ please</td>\n      <td>Non-Abusive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"#train_df.duplicated().sum().item()\nprint(dev_df.duplicated().sum().item())\nprint(train_df.duplicated().sum().item())\ntest_df.duplicated().sum().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:09:58.744783Z","iopub.execute_input":"2025-01-29T17:09:58.745069Z","iopub.status.idle":"2025-01-29T17:09:58.766121Z","shell.execute_reply.started":"2025-01-29T17:09:58.745049Z","shell.execute_reply":"2025-01-29T17:09:58.765523Z"}},"outputs":[{"name":"stdout","text":"0\n11\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_df.drop_duplicates(inplace=True)\ndev_df.drop_duplicates(inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:01.463316Z","iopub.execute_input":"2025-01-29T17:10:01.463675Z","iopub.status.idle":"2025-01-29T17:10:01.472947Z","shell.execute_reply.started":"2025-01-29T17:10:01.463650Z","shell.execute_reply":"2025-01-29T17:10:01.472174Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(dev_df.duplicated().sum().item())\ntrain_df.duplicated().sum().item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:03.614296Z","iopub.execute_input":"2025-01-29T17:10:03.614643Z","iopub.status.idle":"2025-01-29T17:10:03.626025Z","shell.execute_reply.started":"2025-01-29T17:10:03.614618Z","shell.execute_reply":"2025-01-29T17:10:03.625107Z"}},"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Count labels\nnum_labels = train_df['Class'].value_counts()\nprint(num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:06.150042Z","iopub.execute_input":"2025-01-29T17:10:06.150439Z","iopub.status.idle":"2025-01-29T17:10:06.161404Z","shell.execute_reply.started":"2025-01-29T17:10:06.150405Z","shell.execute_reply":"2025-01-29T17:10:06.160393Z"}},"outputs":[{"name":"stdout","text":"Class\nNon-Abusive    1421\nAbusive        1357\nabusive           1\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Change class label from 'abusive' to 'Abusive'\ntrain_df['Class'] = train_df['Class'].replace('abusive', 'Abusive')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:09.379790Z","iopub.execute_input":"2025-01-29T17:10:09.380096Z","iopub.status.idle":"2025-01-29T17:10:09.384947Z","shell.execute_reply.started":"2025-01-29T17:10:09.380074Z","shell.execute_reply":"2025-01-29T17:10:09.384042Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Count labels\nnum_labels = train_df['Class'].value_counts()\nprint(num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:11.306523Z","iopub.execute_input":"2025-01-29T17:10:11.306805Z","iopub.status.idle":"2025-01-29T17:10:11.312884Z","shell.execute_reply.started":"2025-01-29T17:10:11.306787Z","shell.execute_reply":"2025-01-29T17:10:11.311885Z"}},"outputs":[{"name":"stdout","text":"Class\nNon-Abusive    1421\nAbusive        1358\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"filtered_train = train_df[lambda x: x[\"Text\"].str.contains(\"[A-Za-z0-9]\")]\nprint(filtered_train.shape)\nfiltered_dev = dev_df[lambda x: x[\"Text\"].str.contains(\"[A-Za-z0-9]\")]\nprint(filtered_dev.shape)\nfiltered_test = test_df[lambda x: x[\"Text\"].str.contains(\"[A-Za-z0-9]\")]\nprint(filtered_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:13.842452Z","iopub.execute_input":"2025-01-29T17:10:13.842805Z","iopub.status.idle":"2025-01-29T17:10:13.857952Z","shell.execute_reply.started":"2025-01-29T17:10:13.842778Z","shell.execute_reply":"2025-01-29T17:10:13.857157Z"}},"outputs":[{"name":"stdout","text":"(736, 2)\n(161, 2)\n(155, 3)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":" **Preprocessing**","metadata":{}},{"cell_type":"code","source":"def text_to_word_list(text):\n    text = text.split()\n    return text\n\ndef replace_strings(text):\n    emoj = re.compile(\"[\"         # this emoj is to remove all emojis\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\"\n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           u\"\\u00C0-\\u017F\"          #latin\n                           u\"\\u2000-\\u206F\"          #generalPunctuations\n\n                           \"]+\", flags=re.UNICODE)\n    #english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n\n    text=emoji_pattern.sub(r'', text)\n    text=emoj.sub(r'',text)\n    text=text.lower()\n    text=re.sub(r'\\s+', ' ', text)\n\n    text = BeautifulSoup(text, 'html.parser').get_text()\n    text = re.sub(r'(https|http|www)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', ' ', text, flags=re.MULTILINE)\n\n    text = text.replace('\\n', ' ')\n    text = text.replace('тАФ', ' ')\n    text = text.replace('_', ' ')\n    text = text.replace('\\r', ' ')\n    text = re.sub(r'\\\\', ' ',text)\n    # Stopword Removing\n\n    #text=english_pattern.sub(r'', text)\n    #def remove_emojis(text):\n    #text= emoji.get_emoji_regexp().sub(r'', text)\n\n    return text\n\ndef remove_punctuations(my_str):\n    # define punctuation\n    punctuations = '''````┬г|┬в|├С+-*/=рз│рзжрззрзирзйрзкрзлрзмрзнрзорзптАУтАвред!()-[]{};:'\"тАЬ\\тАЩ,<>./?@#$%^&*_~тАШтАФреетАЭтА░ЁЯдгтЪ╜я╕ПтЬМЁЯШАЁЯШГЁЯШДЁЯШБЁЯШЖЁЯШЕЁЯШВЁЯдгЁЯШКЁЯШЗЁЯЩВЁЯЩГЁЯШЙЁЯШМЁЯШНЁЯШШЁЯШЧЁЯШЩЁЯШЪЁЯШЛЁЯШЫЁЯШЭЁЯШЬЁЯдкЁЯдиЁЯзРЁЯдУЁЯШОЁЯдйЁЯе│ЁЯШПЁЯШТЁЯШЮЁЯШФЁЯШЯЁЯШХЁЯЩБтШ╣я╕ПЁЯШгЁЯШЦЁЯШлЁЯШйЁЯШдЁЯШаЁЯШбЁЯдмЁЯШИЁЯС┐ЁЯТАтШая╕ПЁЯТйЁЯдбЁЯС╣ЁЯС║ЁЯС╗ЁЯС╜ЁЯС╛ЁЯдЦЁЯТЛЁЯТМЁЯТШЁЯТЭЁЯТЦЁЯТЧЁЯТУЁЯТЮЁЯТХЁЯТЯтЭгя╕ПЁЯТФтЭдя╕ПтАНЁЯФеЁЯЦдЁЯдНЁЯТпЁЯТвЁЯТеЁЯТлЁЯТжЁЯТиЁЯХ│я╕ПЁЯТгЁЯТмЁЯСБя╕ПтАНЁЯЧия╕ПЁЯЧия╕ПЁЯЧпя╕ПЁЯТнЁЯТдЁЯСЛЁЯдЪЁЯЦРя╕ПтЬЛЁЯЦЦЁЯСМтЬМя╕ПЁЯдЮЁЯдЯЁЯдШЁЯдЩЁЯСИЁЯСЙЁЯСЖЁЯСЗтШЭя╕ПтЬНя╕ПЁЯСПЁЯСПЁЯП╗ЁЯСПЁЯП╝ЁЯСПЁЯП╜ЁЯСПЁЯП╛ЁЯСПЁЯП┐ЁЯСНЁЯСНЁЯП╗ЁЯСНЁЯП╝ЁЯСНЁЯП╜ЁЯСНЁЯП╛ЁЯСНЁЯП┐ЁЯСОЁЯСОЁЯП╗ЁЯСОЁЯП╝ЁЯСОЁЯП╜ЁЯСОЁЯП╛ЁЯСОЁЯП┐тЬКтЬКЁЯП╗тЬКЁЯП╝тЬКЁЯП╜тЬКЁЯП╛тЬКЁЯП┐ЁЯдЫЁЯдЫЁЯП╗ЁЯдЫЁЯП╝ЁЯдЫЁЯП╜ЁЯдЫЁЯП╛ЁЯдЫЁЯП┐ЁЯдЬЁЯдЬЁЯП╗ЁЯдЬЁЯП╝ЁЯдЬЁЯП╜ЁЯдЬЁЯП╛ЁЯдЬЁЯП┐ЁЯдЭЁЯЩПЁЯЩПЁЯП╗ЁЯЩПЁЯП╝ЁЯЩПЁЯП╜ЁЯЩПЁЯП╛ЁЯЩПЁЯП┐тЬНя╕ПЁЯТЕЁЯТЕЁЯП╗ЁЯТЕЁЯП╝ЁЯТЕЁЯП╜ЁЯТЕЁЯП╛ЁЯТЕЁЯП┐ЁЯд│ЁЯТкЁЯТкЁЯП╗ЁЯТкЁЯП╝ЁЯТкЁЯП╜ЁЯТкЁЯП╛ЁЯТкЁЯП┐ЁЯж╛ЁЯж╡ЁЯж┐ЁЯж╢ЁЯСВЁЯСВЁЯП╗ЁЯСВЁЯП╝ЁЯСВЁЯП╜ЁЯСВЁЯП╛ЁЯСВЁЯП┐ЁЯСГЁЯСГЁЯП╗ЁЯСГЁЯП╝ЁЯСГЁЯП╜ЁЯСГЁЯП╛ЁЯСГЁЯП┐ЁЯСАЁЯзаЁЯлАЁЯлБЁЯж╖ЁЯж┤ЁЯСЕЁЯСДЁЯж╗ЁЯж╗ЁЯП╗ЁЯж╗ЁЯП╝ЁЯж╗ЁЯП╜ЁЯж╗ЁЯП╛ЁЯж╗ЁЯП┐ЁЯС╢ЁЯС╢ЁЯП╗ЁЯС╢ЁЯП╝ЁЯС╢ЁЯП╜ЁЯС╢ЁЯП╛ЁЯС╢ЁЯП┐ЁЯзТЁЯзТЁЯП╗ЁЯзТЁЯП╝ЁЯзТЁЯП╜ЁЯзТЁЯП╛ЁЯзТЁЯП┐ЁЯСжЁЯСжЁЯП╗ЁЯСжЁЯП╝ЁЯСжЁЯП╜ЁЯСжЁЯП╛ЁЯСжЁЯП┐ЁЯСзЁЯСзЁЯП╗ЁЯСзЁЯП╝ЁЯСзЁЯП╜ЁЯСзЁЯП╛ЁЯСзЁЯП┐ЁЯзСЁЯзСЁЯП╗ЁЯзСЁЯП╝ЁЯзСЁЯП╜ЁЯзСЁЯП╛ЁЯзСЁЯП┐ЁЯСиЁЯСиЁЯП╗ЁЯСиЁЯП╝ЁЯСиЁЯП╜ЁЯСиЁЯП╛ЁЯСиЁЯП┐ЁЯСйЁЯСйЁЯП╗ЁЯСйЁЯП╝ЁЯСйЁЯП╜ЁЯСйЁЯП╛ЁЯСйЁЯП┐ЁЯзУЁЯзУЁЯП╗ЁЯзУЁЯП╝ЁЯзУЁЯП╜ЁЯзУЁЯП╛ЁЯзУЁЯП┐ЁЯС┤ЁЯС┤ЁЯП╗ЁЯС┤ЁЯП╝ЁЯС┤ЁЯП╜ЁЯС┤ЁЯП╛ЁЯС┤ЁЯП┐ЁЯС╡ЁЯС╡ЁЯП╗ЁЯС╡ЁЯП╝ЁЯС╡ЁЯП╜ЁЯС╡ЁЯП╛ЁЯС╡ЁЯП┐ЁЯСйтАНЁЯж░ЁЯСйЁЯП╗тАНЁЯж░ЁЯСйЁЯП╝тАНЁЯж░ЁЯСйЁЯП╜тАНЁЯж░ЁЯСйЁЯП╛тАНЁЯж░ЁЯСйЁЯП┐тАНЁЯж░ЁЯСйтАНЁЯж▒ЁЯСйЁЯП╗тАНЁЯж▒ЁЯСйЁЯП╝тАНЁЯж▒ЁЯСйЁЯП╜тАНЁЯж▒ЁЯСйЁЯП╛тАНЁЯж▒ЁЯСйЁЯП┐тАНЁЯж▒ЁЯСйтАНЁЯж│ЁЯСйЁЯП╗тАНЁЯж│ЁЯСйЁЯП╝тАНЁЯж│ЁЯСйЁЯП╜тАНЁЯж│ЁЯСйЁЯП╛тАНЁЯж│ЁЯСйЁЯП┐тАНЁЯж│ЁЯСйтАНЁЯж▓ЁЯСйЁЯП╗тАНЁЯж▓ЁЯСйЁЯП╝тАНЁЯж▓ЁЯСйЁЯП╜тАНЁЯж▓ЁЯСйЁЯП╛тАНЁЯж▓ЁЯСйЁЯП┐тАНЁЯж▓ЁЯСитАНЁЯж░ЁЯСиЁЯП╗тАНЁЯж░ЁЯСиЁЯП╝тАНЁЯж░ЁЯСиЁЯП╜тАНЁЯж░ЁЯСиЁЯП╛тАНЁЯж░ЁЯСиЁЯП┐тАНЁЯж░ЁЯСитАНЁЯж▒ЁЯСиЁЯП╗тАНЁЯж▒ЁЯСиЁЯП╝тАНЁЯж▒ЁЯСиЁЯП╜тАНЁЯж▒ЁЯСиЁЯП╛тАНЁЯж▒ЁЯСиЁЯП┐тАНЁЯж▒ЁЯСитАНЁЯж│ЁЯСиЁЯП╗тАНЁЯж│ЁЯСиЁЯП╝ЁЯСиЁЯП╜тАНЁЯж│ЁЯСиЁЯП╛тАНЁЯж│ЁЯСиЁЯП┐тАНЁЯж│ЁЯСитАНЁЯж▓ЁЯСиЁЯП╗тАНЁЯж▓ЁЯСиЁЯП╝тАНЁЯж▓ЁЯСиЁЯП╜тАНЁЯж▓ЁЯСиЁЯП╛тАНЁЯж▓ЁЯСиЁЯП┐тАНЁЯж▓ЁЯж░ЁЯж▒ЁЯж│ЁЯж▓ЁЯС▒тАНтЩАя╕ПЁЯС▒ЁЯП╗тАНтЩАя╕ПтЭдя╕ПтАНЁЯй╣тЭдя╕ПтАНЁЯФетЭдя╕ПтАНЁЯй╣ЁЯзбЁЯТЫЁЯТЪЁЯТЩЁЯТЬЁЯдОя┐╜я┐░рз╖я┐░'''\n    #punctuations = '|┬в|├С+-рз│рзжрззрзирзйрзкрзлрзмрзнрзорзпред()-[]{}<>@#$%^&*_~тАФрееЁЯдгтЪ╜я╕ПтЬМЁЯШАЁЯТЙя┐╜я┐░рз╖я┐░'\n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuations:\n            no_punct = no_punct + char\n\n    # display the unpunctuated string\n    return no_punct\n\n\n\ndef joining(text):\n    out=' '.join(text)\n    return out\n\ndef preprocessing(text):\n    out=remove_punctuations(replace_strings(text))\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:17.955423Z","iopub.execute_input":"2025-01-29T17:10:17.955722Z","iopub.status.idle":"2025-01-29T17:10:17.964220Z","shell.execute_reply.started":"2025-01-29T17:10:17.955702Z","shell.execute_reply":"2025-01-29T17:10:17.963405Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Apply preprocessing to all datasets\ntrain_df['cleanText'] = train_df['Text'].apply(lambda x: preprocessing(str(x)))\ntest_df['cleanText'] = test_df['Text'].apply(lambda x: preprocessing(str(x)))\ndev_df['cleanText'] = dev_df['Text'].apply(lambda x: preprocessing(str(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:22.307810Z","iopub.execute_input":"2025-01-29T17:10:22.308109Z","iopub.status.idle":"2025-01-29T17:10:22.839950Z","shell.execute_reply.started":"2025-01-29T17:10:22.308075Z","shell.execute_reply":"2025-01-29T17:10:22.839255Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"test_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:27.120674Z","iopub.execute_input":"2025-01-29T17:10:27.120959Z","iopub.status.idle":"2025-01-29T17:10:27.131752Z","shell.execute_reply.started":"2025-01-29T17:10:27.120939Z","shell.execute_reply":"2025-01-29T17:10:27.130626Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"      id                                               Text        Class  \\\n0      1  роЗро╡ роТро░рпБ рооро╛ройрпЖроЩрпНроХрпЖроЯрпНроЯ рокрпКро▒рпБроХрпНроХро┐. роТро░рпЗ роТро░рпБ routine роТ...      Abusive   \n1      2  роЗрокрпНроЯро┐ропрпЗ рокрпЗроЪро┐роХрпНроХро┐роЯрпНроЯрпЗ роЗро░рпБроирпНродро╛ роОрокрпНроЯро┐..... ропро╛ро░рпБ рок...  Non-Abusive   \n2      3  роЕроЯроХрпН роХроЯро╡рпБро│рпЗ роЗродрпБ роОройрпНройроХрпН роХрпКроЯрпБроорпИропрпИ роКро░рпБро▓ роЙро▓роХродрпНродрпБро▓ ...  Non-Abusive   \n3      4  роЗродро▒рпНроХрпБ роТро░рпБ родрпАро░рпНро╡рпБ роЗро░рпБроХрпНроХрпБ.  роЕро╡ройрпН роЕро╡ройрпН ро╡рпЗро▓рпИ роЕро╡рой...  Non-Abusive   \n4      5  родроорпНрокро┐ рокрпЛропрпН роиро▓рпНро▓ро╡роЩрпНроХро│рпИ рокрпЗроЯрпНроЯро┐роОроЯрпБ роЕро╡ роЪрпКро▓рпНро╡родрпБ роЕродрпН...      Abusive   \n..   ...                                                ...          ...   \n593  594          роЗродрпБ рокрпИродро┐ропрооро╛ роЗро▓рпНро▓рпИ роиро╛роо рокрпИродро┐ропрооро╛ роорпБроЯро┐ропро▓ роЪро╛рооро┐  Non-Abusive   \n594  595     роЗро░рогрпНроЯрпБ ро╡ро▓рпНроХро░рпН роЪро╛роХрпНроХроЯрпИ. роХроЯрпНроЯро┐ ро╡роЪрпНроЪро┐ роЕроЯро┐роХрпНроХрогрпБроорпН.      Abusive   \n595  596  роЙройрпНроХро┐роЯрпНроЯ родрокрпНрокро┐роЪрпНроЪ роХро╛ро░рпНродрпНродро┐ роЪроирпНродрпЛроЪрооро╛ роЗро░рпБроХрпНроХро╛ройрпН....  Non-Abusive   \n596  597       роХро╛ро░рпНродрпНродро┐ роЕрогрпНроЯрпН родро┐ро╡рпНропро╛ роТро░рпБ ро╖рпЛ рокрогрпНрогрпБроЩрпНроХ ро╡рпЖропро┐роЯрпН  Non-Abusive   \n597  598  роЪрпБроХроирпНродро┐роХрпНроХрпБ роХро╛ро░рпНродрпНродро┐ роЗродрпБроХрпНроХрпБроорпН роОройрпНрой роЪроорпНрокроирпНродроорпН ...  Non-Abusive   \n\n                                             cleanText  \n0    роЗро╡ роТро░рпБ рооро╛ройрпЖроЩрпНроХрпЖроЯрпНроЯ рокрпКро▒рпБроХрпНроХро┐ роТро░рпЗ роТро░рпБ routine роТро░...  \n1    роЗрокрпНроЯро┐ропрпЗ рокрпЗроЪро┐роХрпНроХро┐роЯрпНроЯрпЗ роЗро░рпБроирпНродро╛ роОрокрпНроЯро┐ ропро╛ро░рпБ рокрпЖро░рпБроЪрпБ...  \n2    роЕроЯроХрпН роХроЯро╡рпБро│рпЗ роЗродрпБ роОройрпНройроХрпН роХрпКроЯрпБроорпИропрпИ роКро░рпБро▓ роЙро▓роХродрпНродрпБро▓ ...  \n3    роЗродро▒рпНроХрпБ роТро░рпБ родрпАро░рпНро╡рпБ роЗро░рпБроХрпНроХрпБ роЕро╡ройрпН роЕро╡ройрпН ро╡рпЗро▓рпИ роЕро╡ройрпН ...  \n4    родроорпНрокро┐ рокрпЛропрпН роиро▓рпНро▓ро╡роЩрпНроХро│рпИ рокрпЗроЯрпНроЯро┐роОроЯрпБ роЕро╡ роЪрпКро▓рпНро╡родрпБ роЕродрпН...  \n..                                                 ...  \n593          роЗродрпБ рокрпИродро┐ропрооро╛ роЗро▓рпНро▓рпИ роиро╛роо рокрпИродро┐ропрооро╛ роорпБроЯро┐ропро▓ роЪро╛рооро┐  \n594       роЗро░рогрпНроЯрпБ ро╡ро▓рпНроХро░рпН роЪро╛роХрпНроХроЯрпИ роХроЯрпНроЯро┐ ро╡роЪрпНроЪро┐ роЕроЯро┐роХрпНроХрогрпБроорпН  \n595  роЙройрпНроХро┐роЯрпНроЯ родрокрпНрокро┐роЪрпНроЪ роХро╛ро░рпНродрпНродро┐ роЪроирпНродрпЛроЪрооро╛ роЗро░рпБроХрпНроХро╛ройрпН ...  \n596       роХро╛ро░рпНродрпНродро┐ роЕрогрпНроЯрпН родро┐ро╡рпНропро╛ роТро░рпБ ро╖рпЛ рокрогрпНрогрпБроЩрпНроХ ро╡рпЖропро┐роЯрпН  \n597  роЪрпБроХроирпНродро┐роХрпНроХрпБ роХро╛ро░рпНродрпНродро┐ роЗродрпБроХрпНроХрпБроорпН роОройрпНрой роЪроорпНрокроирпНродроорпН ...  \n\n[598 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Text</th>\n      <th>Class</th>\n      <th>cleanText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>роЗро╡ роТро░рпБ рооро╛ройрпЖроЩрпНроХрпЖроЯрпНроЯ рокрпКро▒рпБроХрпНроХро┐. роТро░рпЗ роТро░рпБ routine роТ...</td>\n      <td>Abusive</td>\n      <td>роЗро╡ роТро░рпБ рооро╛ройрпЖроЩрпНроХрпЖроЯрпНроЯ рокрпКро▒рпБроХрпНроХро┐ роТро░рпЗ роТро░рпБ routine роТро░...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>роЗрокрпНроЯро┐ропрпЗ рокрпЗроЪро┐роХрпНроХро┐роЯрпНроЯрпЗ роЗро░рпБроирпНродро╛ роОрокрпНроЯро┐..... ропро╛ро░рпБ рок...</td>\n      <td>Non-Abusive</td>\n      <td>роЗрокрпНроЯро┐ропрпЗ рокрпЗроЪро┐роХрпНроХро┐роЯрпНроЯрпЗ роЗро░рпБроирпНродро╛ роОрокрпНроЯро┐ ропро╛ро░рпБ рокрпЖро░рпБроЪрпБ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>роЕроЯроХрпН роХроЯро╡рпБро│рпЗ роЗродрпБ роОройрпНройроХрпН роХрпКроЯрпБроорпИропрпИ роКро░рпБро▓ роЙро▓роХродрпНродрпБро▓ ...</td>\n      <td>Non-Abusive</td>\n      <td>роЕроЯроХрпН роХроЯро╡рпБро│рпЗ роЗродрпБ роОройрпНройроХрпН роХрпКроЯрпБроорпИропрпИ роКро░рпБро▓ роЙро▓роХродрпНродрпБро▓ ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>роЗродро▒рпНроХрпБ роТро░рпБ родрпАро░рпНро╡рпБ роЗро░рпБроХрпНроХрпБ.  роЕро╡ройрпН роЕро╡ройрпН ро╡рпЗро▓рпИ роЕро╡рой...</td>\n      <td>Non-Abusive</td>\n      <td>роЗродро▒рпНроХрпБ роТро░рпБ родрпАро░рпНро╡рпБ роЗро░рпБроХрпНроХрпБ роЕро╡ройрпН роЕро╡ройрпН ро╡рпЗро▓рпИ роЕро╡ройрпН ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>родроорпНрокро┐ рокрпЛропрпН роиро▓рпНро▓ро╡роЩрпНроХро│рпИ рокрпЗроЯрпНроЯро┐роОроЯрпБ роЕро╡ роЪрпКро▓рпНро╡родрпБ роЕродрпН...</td>\n      <td>Abusive</td>\n      <td>родроорпНрокро┐ рокрпЛропрпН роиро▓рпНро▓ро╡роЩрпНроХро│рпИ рокрпЗроЯрпНроЯро┐роОроЯрпБ роЕро╡ роЪрпКро▓рпНро╡родрпБ роЕродрпН...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>593</th>\n      <td>594</td>\n      <td>роЗродрпБ рокрпИродро┐ропрооро╛ роЗро▓рпНро▓рпИ роиро╛роо рокрпИродро┐ропрооро╛ роорпБроЯро┐ропро▓ роЪро╛рооро┐</td>\n      <td>Non-Abusive</td>\n      <td>роЗродрпБ рокрпИродро┐ропрооро╛ роЗро▓рпНро▓рпИ роиро╛роо рокрпИродро┐ропрооро╛ роорпБроЯро┐ропро▓ роЪро╛рооро┐</td>\n    </tr>\n    <tr>\n      <th>594</th>\n      <td>595</td>\n      <td>роЗро░рогрпНроЯрпБ ро╡ро▓рпНроХро░рпН роЪро╛роХрпНроХроЯрпИ. роХроЯрпНроЯро┐ ро╡роЪрпНроЪро┐ роЕроЯро┐роХрпНроХрогрпБроорпН.</td>\n      <td>Abusive</td>\n      <td>роЗро░рогрпНроЯрпБ ро╡ро▓рпНроХро░рпН роЪро╛роХрпНроХроЯрпИ роХроЯрпНроЯро┐ ро╡роЪрпНроЪро┐ роЕроЯро┐роХрпНроХрогрпБроорпН</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>596</td>\n      <td>роЙройрпНроХро┐роЯрпНроЯ родрокрпНрокро┐роЪрпНроЪ роХро╛ро░рпНродрпНродро┐ роЪроирпНродрпЛроЪрооро╛ роЗро░рпБроХрпНроХро╛ройрпН....</td>\n      <td>Non-Abusive</td>\n      <td>роЙройрпНроХро┐роЯрпНроЯ родрокрпНрокро┐роЪрпНроЪ роХро╛ро░рпНродрпНродро┐ роЪроирпНродрпЛроЪрооро╛ роЗро░рпБроХрпНроХро╛ройрпН ...</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>597</td>\n      <td>роХро╛ро░рпНродрпНродро┐ роЕрогрпНроЯрпН родро┐ро╡рпНропро╛ роТро░рпБ ро╖рпЛ рокрогрпНрогрпБроЩрпНроХ ро╡рпЖропро┐роЯрпН</td>\n      <td>Non-Abusive</td>\n      <td>роХро╛ро░рпНродрпНродро┐ роЕрогрпНроЯрпН родро┐ро╡рпНропро╛ роТро░рпБ ро╖рпЛ рокрогрпНрогрпБроЩрпНроХ ро╡рпЖропро┐роЯрпН</td>\n    </tr>\n    <tr>\n      <th>597</th>\n      <td>598</td>\n      <td>роЪрпБроХроирпНродро┐роХрпНроХрпБ роХро╛ро░рпНродрпНродро┐ роЗродрпБроХрпНроХрпБроорпН роОройрпНрой роЪроорпНрокроирпНродроорпН ...</td>\n      <td>Non-Abusive</td>\n      <td>роЪрпБроХроирпНродро┐роХрпНроХрпБ роХро╛ро░рпНродрпНродро┐ роЗродрпБроХрпНроХрпБроорпН роОройрпНрой роЪроорпНрокроирпНродроорпН ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>598 rows ├Ч 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"pip install indic-transliteration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:10:32.618173Z","iopub.execute_input":"2025-01-29T17:10:32.618524Z","iopub.status.idle":"2025-01-29T17:10:37.609264Z","shell.execute_reply.started":"2025-01-29T17:10:32.618497Z","shell.execute_reply":"2025-01-29T17:10:37.607893Z"}},"outputs":[{"name":"stdout","text":"Collecting indic-transliteration\n  Downloading indic_transliteration-2.3.69-py3-none-any.whl.metadata (1.4 kB)\nCollecting backports.functools-lru-cache (from indic-transliteration)\n  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from indic-transliteration) (2024.9.11)\nRequirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from indic-transliteration) (0.12.5)\nRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from indic-transliteration) (0.10.2)\nCollecting roman (from indic-transliteration)\n  Downloading roman-5.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer->indic-transliteration) (8.1.7)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from typer->indic-transliteration) (4.12.2)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->indic-transliteration) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer->indic-transliteration) (13.8.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\nDownloading indic_transliteration-2.3.69-py3-none-any.whl (155 kB)\n\u001b[2K   \u001b[90mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m \u001b[32m155.6/155.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\nDownloading roman-5.0-py3-none-any.whl (5.5 kB)\nInstalling collected packages: roman, backports.functools-lru-cache, indic-transliteration\nSuccessfully installed backports.functools-lru-cache-2.0.0 indic-transliteration-2.3.69 roman-5.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"**Converting Code-mix Text into Tamil Text**","metadata":{}},{"cell_type":"code","source":"import unicodedata\nfrom indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\n\n# Function for Unicode normalization and transliteration\ndef normalize_and_transliterate(text):\n    try:\n        # Step 1: Normalize Unicode\n        normalized = unicodedata.normalize('NFKC', text)\n        \n        # Step 2: Transliterate English words to Tamil\n        transliterated = transliterate(normalized, sanscript.ITRANS, sanscript.TAMIL)\n        \n        return transliterated\n    except Exception as e:\n        print(f\"Error processing text: {e}\")\n        return text  # Return original text if any error occurs\n\n# Apply the normalization and transliteration to the 'cleanText' column\ntrain_df['cleanText'] = train_df['cleanText'].apply(normalize_and_transliterate)\ndev_df['cleanText'] = dev_df['cleanText'].apply(normalize_and_transliterate)\ntest_df['cleanText'] = test_df['cleanText'].apply(normalize_and_transliterate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:01.353691Z","iopub.execute_input":"2025-01-29T17:11:01.353991Z","iopub.status.idle":"2025-01-29T17:11:02.286490Z","shell.execute_reply.started":"2025-01-29T17:11:01.353970Z","shell.execute_reply":"2025-01-29T17:11:02.285806Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:07.146224Z","iopub.execute_input":"2025-01-29T17:11:07.146576Z","iopub.status.idle":"2025-01-29T17:11:07.157450Z","shell.execute_reply.started":"2025-01-29T17:11:07.146549Z","shell.execute_reply":"2025-01-29T17:11:07.156617Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                                   Text        Class  \\\n0          роЗродро▓рпНро▓роорпН роТро░рпБ родрпАро░рпНрокрпНрокрпБ роиро╛роЯрпНроЯро╛роорпИ родрпАро░рпНрокрпНрокрпИ рооро▒рпНро▒рпБ  Non-Abusive   \n1              ропро╛ро░рпБроЯро╛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐, роирпА роОроЩрпНроХроЯро╛ роЗро░рпБроХрпНроХ ?  Non-Abusive   \n2     роЗро░рогрпНроЯрпБ рокрпЗро░ро┐ройрпН (рокрпБрогрпНроЯрпИропрпИ) роЪро╛рооро╛ройрпНроХро│рпИ роХрпЛрогро┐роп роКроЪро╛ро▓рпН...      Abusive   \n3     роОройрпНрой родро┐рооро┐ро░рпН роЗроирпНрод рокрпКрогрпНрогрпБроХрпНроХрпБ.....роороХрпНроХро│рпЗ роЗройрпНройрпБроорпН...      Abusive   \n4             роРропрпЛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐роХрпН роО роХро╛роЯрпНроЯрпБроЩрпНроХ рокро╛ please  Non-Abusive   \n...                                                 ...          ...   \n2785  ро░рпЖрогрпНроЯрпБроорпЗ роЕро╡ройро╛ роЗро▓рпНро▓ роЕро╡ро│ро╛....роТро░рпЗ роХройрпНроГрокрпНропрпВроЪройрпН.......      Abusive   \n2786  ро░ро╛роЬрпЗро╖рпНро╡ро░ро┐ роЪрпЖроорпНрооропро╛ роЗро░рпБроХрпНроХрпБ ро░рпКроорпНрок роХрпБро│рпЛроЪрокрпНро▓ рокро╛ро░рпНроХ...      Abusive   \n2787  роОройроХрпНроХрпБ родрпЖро▒ро┐роЪрпНроЪрпБ роХро╛ро░рпНродрпНродро┐ ройрпБ роТро░рпБродрпНродройрпН роиро▓рпНро▓ро╛ роУродрпН...      Abusive   \n2788  роирпАроЩрпНроХро│рпН роХро╡ро▓рпИ роХрпКро▓рпНро▓ро╛родрпАро░рпНроХро│рпН роЪроХрпЛродро░ро┐. роЪро┐ройрпНроорпИ роОро╡рпНро│...      Abusive   \n2789  30 lakhs views and 9k comments роЕроЯ рокро╛ро╡ро┐роЩрпНроХро│ро╛ роЕро╡...  Non-Abusive   \n\n                                              cleanText  \n0          роЗродро▓рпНро▓роорпН роТро░рпБ родрпАро░рпНрокрпНрокрпБ роиро╛роЯрпНроЯро╛роорпИ родрпАро░рпНрокрпНрокрпИ рооро▒рпНро▒рпБ  \n1                ропро╛ро░рпБроЯро╛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐ роирпА роОроЩрпНроХроЯро╛ роЗро░рпБроХрпНроХ   \n2     роЗро░рогрпНроЯрпБ рокрпЗро░ро┐ройрпН рокрпБрогрпНроЯрпИропрпИ роЪро╛рооро╛ройрпНроХро│рпИ роХрпЛрогро┐роп роКроЪро╛ро▓рпН род...  \n3     роОройрпНрой родро┐рооро┐ро░рпН роЗроирпНрод рокрпКрогрпНрогрпБроХрпНроХрпБроороХрпНроХро│рпЗ роЗройрпНройрпБроорпН роЗро╡ро│рпБ...  \n4            роРропрпЛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐роХрпН роО роХро╛роЯрпНроЯрпБроЩрпНроХ рокро╛ рокрпНро▓рпЗроЕро╕рпЗ  \n...                                                 ...  \n2785  ро░рпЖрогрпНроЯрпБроорпЗ роЕро╡ройро╛ роЗро▓рпНро▓ роЕро╡ро│ро╛роТро░рпЗ роХройрпНроГрокрпНропрпВроЪройрпН роХро╖рпНроЯроорпН ...  \n2786  ро░ро╛роЬрпЗро╖рпНро╡ро░ро┐ роЪрпЖроорпНрооропро╛ роЗро░рпБроХрпНроХрпБ ро░рпКроорпНрок роХрпБро│рпЛроЪрокрпНро▓ рокро╛ро░рпНроХ...  \n2787  роОройроХрпНроХрпБ родрпЖро▒ро┐роЪрпНроЪрпБ роХро╛ро░рпНродрпНродро┐ ройрпБ роТро░рпБродрпНродройрпН роиро▓рпНро▓ро╛ роУродрпН...  \n2788  роирпАроЩрпНроХро│рпН роХро╡ро▓рпИ роХрпКро▓рпНро▓ро╛родрпАро░рпНроХро│рпН роЪроХрпЛродро░ро┐ роЪро┐ройрпНроорпИ роОро╡рпНро│рпЛ...  \n2789  рпйрпж ро▓роХрпНро╕рпН ро╡ро┐роПро╡рпНро╕рпН роЕроирпНродрпН рппроХрпН роЪрпЛроорпНроорпЗроирпНродрпНро╕рпН роЕроЯ рокро╛ро╡...  \n\n[2779 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Class</th>\n      <th>cleanText</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>роЗродро▓рпНро▓роорпН роТро░рпБ родрпАро░рпНрокрпНрокрпБ роиро╛роЯрпНроЯро╛роорпИ родрпАро░рпНрокрпНрокрпИ рооро▒рпНро▒рпБ</td>\n      <td>Non-Abusive</td>\n      <td>роЗродро▓рпНро▓роорпН роТро░рпБ родрпАро░рпНрокрпНрокрпБ роиро╛роЯрпНроЯро╛роорпИ родрпАро░рпНрокрпНрокрпИ рооро▒рпНро▒рпБ</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ропро╛ро░рпБроЯро╛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐, роирпА роОроЩрпНроХроЯро╛ роЗро░рпБроХрпНроХ ?</td>\n      <td>Non-Abusive</td>\n      <td>ропро╛ро░рпБроЯро╛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐ роирпА роОроЩрпНроХроЯро╛ роЗро░рпБроХрпНроХ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>роЗро░рогрпНроЯрпБ рокрпЗро░ро┐ройрпН (рокрпБрогрпНроЯрпИропрпИ) роЪро╛рооро╛ройрпНроХро│рпИ роХрпЛрогро┐роп роКроЪро╛ро▓рпН...</td>\n      <td>Abusive</td>\n      <td>роЗро░рогрпНроЯрпБ рокрпЗро░ро┐ройрпН рокрпБрогрпНроЯрпИропрпИ роЪро╛рооро╛ройрпНроХро│рпИ роХрпЛрогро┐роп роКроЪро╛ро▓рпН род...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>роОройрпНрой родро┐рооро┐ро░рпН роЗроирпНрод рокрпКрогрпНрогрпБроХрпНроХрпБ.....роороХрпНроХро│рпЗ роЗройрпНройрпБроорпН...</td>\n      <td>Abusive</td>\n      <td>роОройрпНрой родро┐рооро┐ро░рпН роЗроирпНрод рокрпКрогрпНрогрпБроХрпНроХрпБроороХрпНроХро│рпЗ роЗройрпНройрпБроорпН роЗро╡ро│рпБ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>роРропрпЛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐роХрпН роО роХро╛роЯрпНроЯрпБроЩрпНроХ рокро╛ please</td>\n      <td>Non-Abusive</td>\n      <td>роРропрпЛ роЕроирпНрод роХро╛ро░рпНродрпНродро┐роХрпН роО роХро╛роЯрпНроЯрпБроЩрпНроХ рокро╛ рокрпНро▓рпЗроЕро╕рпЗ</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2785</th>\n      <td>ро░рпЖрогрпНроЯрпБроорпЗ роЕро╡ройро╛ роЗро▓рпНро▓ роЕро╡ро│ро╛....роТро░рпЗ роХройрпНроГрокрпНропрпВроЪройрпН.......</td>\n      <td>Abusive</td>\n      <td>ро░рпЖрогрпНроЯрпБроорпЗ роЕро╡ройро╛ роЗро▓рпНро▓ роЕро╡ро│ро╛роТро░рпЗ роХройрпНроГрокрпНропрпВроЪройрпН роХро╖рпНроЯроорпН ...</td>\n    </tr>\n    <tr>\n      <th>2786</th>\n      <td>ро░ро╛роЬрпЗро╖рпНро╡ро░ро┐ роЪрпЖроорпНрооропро╛ роЗро░рпБроХрпНроХрпБ ро░рпКроорпНрок роХрпБро│рпЛроЪрокрпНро▓ рокро╛ро░рпНроХ...</td>\n      <td>Abusive</td>\n      <td>ро░ро╛роЬрпЗро╖рпНро╡ро░ро┐ роЪрпЖроорпНрооропро╛ роЗро░рпБроХрпНроХрпБ ро░рпКроорпНрок роХрпБро│рпЛроЪрокрпНро▓ рокро╛ро░рпНроХ...</td>\n    </tr>\n    <tr>\n      <th>2787</th>\n      <td>роОройроХрпНроХрпБ родрпЖро▒ро┐роЪрпНроЪрпБ роХро╛ро░рпНродрпНродро┐ ройрпБ роТро░рпБродрпНродройрпН роиро▓рпНро▓ро╛ роУродрпН...</td>\n      <td>Abusive</td>\n      <td>роОройроХрпНроХрпБ родрпЖро▒ро┐роЪрпНроЪрпБ роХро╛ро░рпНродрпНродро┐ ройрпБ роТро░рпБродрпНродройрпН роиро▓рпНро▓ро╛ роУродрпН...</td>\n    </tr>\n    <tr>\n      <th>2788</th>\n      <td>роирпАроЩрпНроХро│рпН роХро╡ро▓рпИ роХрпКро▓рпНро▓ро╛родрпАро░рпНроХро│рпН роЪроХрпЛродро░ро┐. роЪро┐ройрпНроорпИ роОро╡рпНро│...</td>\n      <td>Abusive</td>\n      <td>роирпАроЩрпНроХро│рпН роХро╡ро▓рпИ роХрпКро▓рпНро▓ро╛родрпАро░рпНроХро│рпН роЪроХрпЛродро░ро┐ роЪро┐ройрпНроорпИ роОро╡рпНро│рпЛ...</td>\n    </tr>\n    <tr>\n      <th>2789</th>\n      <td>30 lakhs views and 9k comments роЕроЯ рокро╛ро╡ро┐роЩрпНроХро│ро╛ роЕро╡...</td>\n      <td>Non-Abusive</td>\n      <td>рпйрпж ро▓роХрпНро╕рпН ро╡ро┐роПро╡рпНро╕рпН роЕроирпНродрпН рппроХрпН роЪрпЛроорпНроорпЗроирпНродрпНро╕рпН роЕроЯ рокро╛ро╡...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2779 rows ├Ч 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"test_df['Class'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:13.618378Z","iopub.execute_input":"2025-01-29T17:11:13.618662Z","iopub.status.idle":"2025-01-29T17:11:13.625384Z","shell.execute_reply.started":"2025-01-29T17:11:13.618643Z","shell.execute_reply":"2025-01-29T17:11:13.624369Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Class\nAbusive        305\nNon-Abusive    293\nName: count, dtype: int64"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"filtered_train = train_df[lambda x: x[\"cleanText\"].str.contains(\"[A-Za-z0-9]\")]\nprint(filtered_train.shape)\nfiltered_dev = dev_df[lambda x: x[\"cleanText\"].str.contains(\"[A-Za-z0-9]\")]\nprint(filtered_dev.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:18.887648Z","iopub.execute_input":"2025-01-29T17:11:18.887929Z","iopub.status.idle":"2025-01-29T17:11:18.900643Z","shell.execute_reply.started":"2025-01-29T17:11:18.887910Z","shell.execute_reply":"2025-01-29T17:11:18.899630Z"}},"outputs":[{"name":"stdout","text":"(0, 3)\n(0, 3)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"train_df['enc_label'] = train_df['Class'].replace({'Abusive':1, 'Non-Abusive':0})\ndev_df['enc_label'] = dev_df['Class'].replace({'Abusive':1, 'Non-Abusive':0})\ntest_df['enc_label'] = test_df['Class'].replace({'Abusive':1, 'Non-Abusive':0})\ntrain_df.tail(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:21.881799Z","iopub.execute_input":"2025-01-29T17:11:21.882078Z","iopub.status.idle":"2025-01-29T17:11:21.896211Z","shell.execute_reply.started":"2025-01-29T17:11:21.882058Z","shell.execute_reply":"2025-01-29T17:11:21.895466Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                                   Text        Class  \\\n2785  ро░рпЖрогрпНроЯрпБроорпЗ роЕро╡ройро╛ роЗро▓рпНро▓ роЕро╡ро│ро╛....роТро░рпЗ роХройрпНроГрокрпНропрпВроЪройрпН.......      Abusive   \n2786  ро░ро╛роЬрпЗро╖рпНро╡ро░ро┐ роЪрпЖроорпНрооропро╛ роЗро░рпБроХрпНроХрпБ ро░рпКроорпНрок роХрпБро│рпЛроЪрокрпНро▓ рокро╛ро░рпНроХ...      Abusive   \n2787  роОройроХрпНроХрпБ родрпЖро▒ро┐роЪрпНроЪрпБ роХро╛ро░рпНродрпНродро┐ ройрпБ роТро░рпБродрпНродройрпН роиро▓рпНро▓ро╛ роУродрпН...      Abusive   \n2788  роирпАроЩрпНроХро│рпН роХро╡ро▓рпИ роХрпКро▓рпНро▓ро╛родрпАро░рпНроХро│рпН роЪроХрпЛродро░ро┐. роЪро┐ройрпНроорпИ роОро╡рпНро│...      Abusive   \n2789  30 lakhs views and 9k comments роЕроЯ рокро╛ро╡ро┐роЩрпНроХро│ро╛ роЕро╡...  Non-Abusive   \n\n                                              cleanText  enc_label  \n2785  ро░рпЖрогрпНроЯрпБроорпЗ роЕро╡ройро╛ роЗро▓рпНро▓ роЕро╡ро│ро╛роТро░рпЗ роХройрпНроГрокрпНропрпВроЪройрпН роХро╖рпНроЯроорпН ...          1  \n2786  ро░ро╛роЬрпЗро╖рпНро╡ро░ро┐ роЪрпЖроорпНрооропро╛ роЗро░рпБроХрпНроХрпБ ро░рпКроорпНрок роХрпБро│рпЛроЪрокрпНро▓ рокро╛ро░рпНроХ...          1  \n2787  роОройроХрпНроХрпБ родрпЖро▒ро┐роЪрпНроЪрпБ роХро╛ро░рпНродрпНродро┐ ройрпБ роТро░рпБродрпНродройрпН роиро▓рпНро▓ро╛ роУродрпН...          1  \n2788  роирпАроЩрпНроХро│рпН роХро╡ро▓рпИ роХрпКро▓рпНро▓ро╛родрпАро░рпНроХро│рпН роЪроХрпЛродро░ро┐ роЪро┐ройрпНроорпИ роОро╡рпНро│рпЛ...          1  \n2789  рпйрпж ро▓роХрпНро╕рпН ро╡ро┐роПро╡рпНро╕рпН роЕроирпНродрпН рппроХрпН роЪрпЛроорпНроорпЗроирпНродрпНро╕рпН роЕроЯ рокро╛ро╡...          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Class</th>\n      <th>cleanText</th>\n      <th>enc_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2785</th>\n      <td>ро░рпЖрогрпНроЯрпБроорпЗ роЕро╡ройро╛ роЗро▓рпНро▓ роЕро╡ро│ро╛....роТро░рпЗ роХройрпНроГрокрпНропрпВроЪройрпН.......</td>\n      <td>Abusive</td>\n      <td>ро░рпЖрогрпНроЯрпБроорпЗ роЕро╡ройро╛ роЗро▓рпНро▓ роЕро╡ро│ро╛роТро░рпЗ роХройрпНроГрокрпНропрпВроЪройрпН роХро╖рпНроЯроорпН ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2786</th>\n      <td>ро░ро╛роЬрпЗро╖рпНро╡ро░ро┐ роЪрпЖроорпНрооропро╛ роЗро░рпБроХрпНроХрпБ ро░рпКроорпНрок роХрпБро│рпЛроЪрокрпНро▓ рокро╛ро░рпНроХ...</td>\n      <td>Abusive</td>\n      <td>ро░ро╛роЬрпЗро╖рпНро╡ро░ро┐ роЪрпЖроорпНрооропро╛ роЗро░рпБроХрпНроХрпБ ро░рпКроорпНрок роХрпБро│рпЛроЪрокрпНро▓ рокро╛ро░рпНроХ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2787</th>\n      <td>роОройроХрпНроХрпБ родрпЖро▒ро┐роЪрпНроЪрпБ роХро╛ро░рпНродрпНродро┐ ройрпБ роТро░рпБродрпНродройрпН роиро▓рпНро▓ро╛ роУродрпН...</td>\n      <td>Abusive</td>\n      <td>роОройроХрпНроХрпБ родрпЖро▒ро┐роЪрпНроЪрпБ роХро╛ро░рпНродрпНродро┐ ройрпБ роТро░рпБродрпНродройрпН роиро▓рпНро▓ро╛ роУродрпН...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2788</th>\n      <td>роирпАроЩрпНроХро│рпН роХро╡ро▓рпИ роХрпКро▓рпНро▓ро╛родрпАро░рпНроХро│рпН роЪроХрпЛродро░ро┐. роЪро┐ройрпНроорпИ роОро╡рпНро│...</td>\n      <td>Abusive</td>\n      <td>роирпАроЩрпНроХро│рпН роХро╡ро▓рпИ роХрпКро▓рпНро▓ро╛родрпАро░рпНроХро│рпН роЪроХрпЛродро░ро┐ роЪро┐ройрпНроорпИ роОро╡рпНро│рпЛ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2789</th>\n      <td>30 lakhs views and 9k comments роЕроЯ рокро╛ро╡ро┐роЩрпНроХро│ро╛ роЕро╡...</td>\n      <td>Non-Abusive</td>\n      <td>рпйрпж ро▓роХрпНро╕рпН ро╡ро┐роПро╡рпНро╕рпН роЕроирпНродрпН рппроХрпН роЪрпЛроорпНроорпЗроирпНродрпНро╕рпН роЕроЯ рокро╛ро╡...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"test_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:24.681455Z","iopub.execute_input":"2025-01-29T17:11:24.681745Z","iopub.status.idle":"2025-01-29T17:11:24.690762Z","shell.execute_reply.started":"2025-01-29T17:11:24.681725Z","shell.execute_reply":"2025-01-29T17:11:24.689971Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"   id                                               Text        Class  \\\n0   1  роЗро╡ роТро░рпБ рооро╛ройрпЖроЩрпНроХрпЖроЯрпНроЯ рокрпКро▒рпБроХрпНроХро┐. роТро░рпЗ роТро░рпБ routine роТ...      Abusive   \n1   2  роЗрокрпНроЯро┐ропрпЗ рокрпЗроЪро┐роХрпНроХро┐роЯрпНроЯрпЗ роЗро░рпБроирпНродро╛ роОрокрпНроЯро┐..... ропро╛ро░рпБ рок...  Non-Abusive   \n2   3  роЕроЯроХрпН роХроЯро╡рпБро│рпЗ роЗродрпБ роОройрпНройроХрпН роХрпКроЯрпБроорпИропрпИ роКро░рпБро▓ роЙро▓роХродрпНродрпБро▓ ...  Non-Abusive   \n3   4  роЗродро▒рпНроХрпБ роТро░рпБ родрпАро░рпНро╡рпБ роЗро░рпБроХрпНроХрпБ.  роЕро╡ройрпН роЕро╡ройрпН ро╡рпЗро▓рпИ роЕро╡рой...  Non-Abusive   \n4   5  родроорпНрокро┐ рокрпЛропрпН роиро▓рпНро▓ро╡роЩрпНроХро│рпИ рокрпЗроЯрпНроЯро┐роОроЯрпБ роЕро╡ роЪрпКро▓рпНро╡родрпБ роЕродрпН...      Abusive   \n5   6  роЪро┐ройрпНрооропро┐ ро╡рпИро░роорпБродрпНродрпБро╡рпИ рокродрпНродрпБ ро╡ро░рпБроЯроЩрпНроХро│рпБроХрпНроХрпБ роорпБройрпНрокрпБ...      Abusive   \n6   7                     роЗро╡ роТро░рпБ роЕро│ро╡рпБ родро╛ройрпН роЪрокрпНрокро┐ роЗро░рпБроХрпНроХро╛      Abusive   \n7   8  роЙройрпНройрпИ рокрпЛроЯрпНроЯрпЛ роОроЯрпБродрпНродрпБ роХроХрпНроХрпВро╕рпН ро▓ родро╛ройрпН рооро╛роЯрпНроЯро┐ ро╡рпИроХ...      Abusive   \n8   9  ро╡ро╛ро┤рпНроХрпНроХрпИ рооро┐ро▓рпН роТро╡рпНро╡рпКро░рпБро╡ро░рпБроХрпНроХрпБроорпН роЖропро┐ро░роорпН рокро┐ро░роЪрпНроЪро┐рой...  Non-Abusive   \n9  10   роЗроирпНрод роРроЯрпНроЯроорпН роЪрпБроХроирпНродро┐ роЕро╡рпНро╡ро│ро╡рпБ роиро▓рпНро▓ро╡ роХро┐роЯрпИропро╛родрпБ;!!!!!      Abusive   \n\n                                           cleanText  enc_label  \n0  роЗро╡ роТро░рпБ рооро╛ройрпЖроЩрпНроХрпЖроЯрпНроЯ рокрпКро▒рпБроХрпНроХро┐ роТро░рпЗ роТро░рпБ ро░рпЛроЙродро┐роирпЗ роТро░...          1  \n1  роЗрокрпНроЯро┐ропрпЗ рокрпЗроЪро┐роХрпНроХро┐роЯрпНроЯрпЗ роЗро░рпБроирпНродро╛ роОрокрпНроЯро┐ ропро╛ро░рпБ рокрпЖро░рпБроЪрпБ...          0  \n2  роЕроЯроХрпН роХроЯро╡рпБро│рпЗ роЗродрпБ роОройрпНройроХрпН роХрпКроЯрпБроорпИропрпИ роКро░рпБро▓ роЙро▓роХродрпНродрпБро▓ ...          0  \n3  роЗродро▒рпНроХрпБ роТро░рпБ родрпАро░рпНро╡рпБ роЗро░рпБроХрпНроХрпБ роЕро╡ройрпН роЕро╡ройрпН ро╡рпЗро▓рпИ роЕро╡ройрпН ...          0  \n4  родроорпНрокро┐ рокрпЛропрпН роиро▓рпНро▓ро╡роЩрпНроХро│рпИ рокрпЗроЯрпНроЯро┐роОроЯрпБ роЕро╡ роЪрпКро▓рпНро╡родрпБ роЕродрпН...          1  \n5  роЪро┐ройрпНрооропро┐ ро╡рпИро░роорпБродрпНродрпБро╡рпИ рокродрпНродрпБ ро╡ро░рпБроЯроЩрпНроХро│рпБроХрпНроХрпБ роорпБройрпНрокрпБ...          1  \n6                     роЗро╡ роТро░рпБ роЕро│ро╡рпБ родро╛ройрпН роЪрокрпНрокро┐ роЗро░рпБроХрпНроХро╛          1  \n7  роЙройрпНройрпИ рокрпЛроЯрпНроЯрпЛ роОроЯрпБродрпНродрпБ роХроХрпНроХрпВро╕рпН ро▓ родро╛ройрпН рооро╛роЯрпНроЯро┐ ро╡рпИроХ...          1  \n8  ро╡ро╛ро┤рпНроХрпНроХрпИ рооро┐ро▓рпН роТро╡рпНро╡рпКро░рпБро╡ро░рпБроХрпНроХрпБроорпН роЖропро┐ро░роорпН рокро┐ро░роЪрпНроЪро┐рой...          0  \n9         роЗроирпНрод роРроЯрпНроЯроорпН роЪрпБроХроирпНродро┐ роЕро╡рпНро╡ро│ро╡рпБ роиро▓рпНро▓ро╡ роХро┐роЯрпИропро╛родрпБ          1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Text</th>\n      <th>Class</th>\n      <th>cleanText</th>\n      <th>enc_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>роЗро╡ роТро░рпБ рооро╛ройрпЖроЩрпНроХрпЖроЯрпНроЯ рокрпКро▒рпБроХрпНроХро┐. роТро░рпЗ роТро░рпБ routine роТ...</td>\n      <td>Abusive</td>\n      <td>роЗро╡ роТро░рпБ рооро╛ройрпЖроЩрпНроХрпЖроЯрпНроЯ рокрпКро▒рпБроХрпНроХро┐ роТро░рпЗ роТро░рпБ ро░рпЛроЙродро┐роирпЗ роТро░...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>роЗрокрпНроЯро┐ропрпЗ рокрпЗроЪро┐роХрпНроХро┐роЯрпНроЯрпЗ роЗро░рпБроирпНродро╛ роОрокрпНроЯро┐..... ропро╛ро░рпБ рок...</td>\n      <td>Non-Abusive</td>\n      <td>роЗрокрпНроЯро┐ропрпЗ рокрпЗроЪро┐роХрпНроХро┐роЯрпНроЯрпЗ роЗро░рпБроирпНродро╛ роОрокрпНроЯро┐ ропро╛ро░рпБ рокрпЖро░рпБроЪрпБ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>роЕроЯроХрпН роХроЯро╡рпБро│рпЗ роЗродрпБ роОройрпНройроХрпН роХрпКроЯрпБроорпИропрпИ роКро░рпБро▓ роЙро▓роХродрпНродрпБро▓ ...</td>\n      <td>Non-Abusive</td>\n      <td>роЕроЯроХрпН роХроЯро╡рпБро│рпЗ роЗродрпБ роОройрпНройроХрпН роХрпКроЯрпБроорпИропрпИ роКро░рпБро▓ роЙро▓роХродрпНродрпБро▓ ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>роЗродро▒рпНроХрпБ роТро░рпБ родрпАро░рпНро╡рпБ роЗро░рпБроХрпНроХрпБ.  роЕро╡ройрпН роЕро╡ройрпН ро╡рпЗро▓рпИ роЕро╡рой...</td>\n      <td>Non-Abusive</td>\n      <td>роЗродро▒рпНроХрпБ роТро░рпБ родрпАро░рпНро╡рпБ роЗро░рпБроХрпНроХрпБ роЕро╡ройрпН роЕро╡ройрпН ро╡рпЗро▓рпИ роЕро╡ройрпН ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>родроорпНрокро┐ рокрпЛропрпН роиро▓рпНро▓ро╡роЩрпНроХро│рпИ рокрпЗроЯрпНроЯро┐роОроЯрпБ роЕро╡ роЪрпКро▓рпНро╡родрпБ роЕродрпН...</td>\n      <td>Abusive</td>\n      <td>родроорпНрокро┐ рокрпЛропрпН роиро▓рпНро▓ро╡роЩрпНроХро│рпИ рокрпЗроЯрпНроЯро┐роОроЯрпБ роЕро╡ роЪрпКро▓рпНро╡родрпБ роЕродрпН...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>роЪро┐ройрпНрооропро┐ ро╡рпИро░роорпБродрпНродрпБро╡рпИ рокродрпНродрпБ ро╡ро░рпБроЯроЩрпНроХро│рпБроХрпНроХрпБ роорпБройрпНрокрпБ...</td>\n      <td>Abusive</td>\n      <td>роЪро┐ройрпНрооропро┐ ро╡рпИро░роорпБродрпНродрпБро╡рпИ рокродрпНродрпБ ро╡ро░рпБроЯроЩрпНроХро│рпБроХрпНроХрпБ роорпБройрпНрокрпБ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>роЗро╡ роТро░рпБ роЕро│ро╡рпБ родро╛ройрпН роЪрокрпНрокро┐ роЗро░рпБроХрпНроХро╛</td>\n      <td>Abusive</td>\n      <td>роЗро╡ роТро░рпБ роЕро│ро╡рпБ родро╛ройрпН роЪрокрпНрокро┐ роЗро░рпБроХрпНроХро╛</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>роЙройрпНройрпИ рокрпЛроЯрпНроЯрпЛ роОроЯрпБродрпНродрпБ роХроХрпНроХрпВро╕рпН ро▓ родро╛ройрпН рооро╛роЯрпНроЯро┐ ро╡рпИроХ...</td>\n      <td>Abusive</td>\n      <td>роЙройрпНройрпИ рокрпЛроЯрпНроЯрпЛ роОроЯрпБродрпНродрпБ роХроХрпНроХрпВро╕рпН ро▓ родро╛ройрпН рооро╛роЯрпНроЯро┐ ро╡рпИроХ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>ро╡ро╛ро┤рпНроХрпНроХрпИ рооро┐ро▓рпН роТро╡рпНро╡рпКро░рпБро╡ро░рпБроХрпНроХрпБроорпН роЖропро┐ро░роорпН рокро┐ро░роЪрпНроЪро┐рой...</td>\n      <td>Non-Abusive</td>\n      <td>ро╡ро╛ро┤рпНроХрпНроХрпИ рооро┐ро▓рпН роТро╡рпНро╡рпКро░рпБро╡ро░рпБроХрпНроХрпБроорпН роЖропро┐ро░роорпН рокро┐ро░роЪрпНроЪро┐рой...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>роЗроирпНрод роРроЯрпНроЯроорпН роЪрпБроХроирпНродро┐ роЕро╡рпНро╡ро│ро╡рпБ роиро▓рпНро▓ро╡ роХро┐роЯрпИропро╛родрпБ;!!!!!</td>\n      <td>Abusive</td>\n      <td>роЗроирпНрод роРроЯрпНроЯроорпН роЪрпБроХроирпНродро┐ роЕро╡рпНро╡ро│ро╡рпБ роиро▓рпНро▓ро╡ роХро┐роЯрпИропро╛родрпБ</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"train_df.enc_label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:27.959729Z","iopub.execute_input":"2025-01-29T17:11:27.960059Z","iopub.status.idle":"2025-01-29T17:11:27.967305Z","shell.execute_reply.started":"2025-01-29T17:11:27.960033Z","shell.execute_reply":"2025-01-29T17:11:27.966445Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"enc_label\n0    1421\n1    1358\nName: count, dtype: int64"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"dev_df.enc_label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:30.486991Z","iopub.execute_input":"2025-01-29T17:11:30.487282Z","iopub.status.idle":"2025-01-29T17:11:30.493601Z","shell.execute_reply.started":"2025-01-29T17:11:30.487261Z","shell.execute_reply":"2025-01-29T17:11:30.492859Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"enc_label\n0    320\n1    278\nName: count, dtype: int64"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_df.enc_label.value_counts()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:32.350742Z","iopub.execute_input":"2025-01-29T17:11:32.351123Z","iopub.status.idle":"2025-01-29T17:11:32.358963Z","shell.execute_reply.started":"2025-01-29T17:11:32.351092Z","shell.execute_reply":"2025-01-29T17:11:32.358125Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"enc_label\n0    1421\n1    1358\nName: count, dtype: int64"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"**ML_Models (LR,DT,RF,SVM)**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n\n# Prepare features and labels\nX_train, y_train = train_df['cleanText'], train_df['enc_label']\nX_dev, y_dev = dev_df['cleanText'], dev_df['enc_label']\nX_test, y_test = test_df['cleanText'], test_df['enc_label']\n\n# Combine training and development sets for better model generalization\nX_combined = pd.concat([X_train, X_dev], ignore_index=True)\ny_combined = pd.concat([y_train, y_dev], ignore_index=True)\n\n# TF-IDF Vectorization\ntfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), stop_words='english')\nX_train_tfidf = tfidf.fit_transform(X_combined)\nX_test_tfidf = tfidf.transform(X_test)\n\n# Initialize models\nmodels = {\n    'Logistic Regression': LogisticRegression(max_iter=1000),\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'Support Vector Machine': SVC(probability=True)\n}\n\n# Train and evaluate each model\nfor model_name, model in models.items():\n    print(f\"\\nTraining {model_name}...\")\n    model.fit(X_train_tfidf, y_combined)\n    \n    # Predictions\n    y_pred = model.predict(X_test_tfidf)\n    \n    # Evaluation\n    precision = precision_score(y_test, y_pred, average='macro')\n    recall = recall_score(y_test, y_pred, average='macro')\n    f1 = f1_score(y_test, y_pred, average='macro')\n\n    print(f\"Results for {model_name}:\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"Macro F1 Score: {f1:.4f}\")\n\n# Hyperparameter tuning example for SVM using GridSearchCV\nparam_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\ngrid_search = GridSearchCV(SVC(), param_grid, cv=3, scoring='accuracy')\ngrid_search.fit(X_train_tfidf, y_combined)\nprint(\"\\nBest parameters for SVM:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T07:40:12.162999Z","iopub.execute_input":"2025-01-29T07:40:12.163336Z","iopub.status.idle":"2025-01-29T07:40:42.516816Z","shell.execute_reply.started":"2025-01-29T07:40:12.163308Z","shell.execute_reply":"2025-01-29T07:40:42.515870Z"}},"outputs":[{"name":"stdout","text":"\nTraining Logistic Regression...\nResults for Logistic Regression:\nPrecision: 0.7047\nRecall: 0.7045\nMacro F1 Score: 0.7040\n\nTraining Decision Tree...\nResults for Decision Tree:\nPrecision: 0.6085\nRecall: 0.6085\nMacro F1 Score: 0.6085\n\nTraining Random Forest...\nResults for Random Forest:\nPrecision: 0.6658\nRecall: 0.6635\nMacro F1 Score: 0.6614\n\nTraining Support Vector Machine...\nResults for Support Vector Machine:\nPrecision: 0.6895\nRecall: 0.6894\nMacro F1 Score: 0.6889\n\nBest parameters for SVM: {'C': 1, 'kernel': 'rbf'}\nBest score: 0.6867024537859351\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"**DL_Models (BiLSTM,CNN + LSTM)**","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Conv1D, MaxPooling1D, Dropout, Flatten\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\n# Parameters\nmax_len = 100\nembedding_dim = 128\n\n# Tokenization and Word2Vec Training\ntokenized_sentences = [sentence.split() for sentence in X_combined]\nword2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=embedding_dim, window=5, min_count=1)\nvocab_size = len(word2vec_model.wv.index_to_key)\n\n# Create Embedding Matrix\nembedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\nword_index = {word: idx for idx, word in enumerate(word2vec_model.wv.index_to_key, 1)}\n\nfor word, idx in word_index.items():\n    if word in word2vec_model.wv:\n        embedding_matrix[idx] = word2vec_model.wv[word]\n\n# Tokenizer Setup for Sequences\ntokenizer = Tokenizer()\ntokenizer.word_index = word_index\n\nX_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_combined), maxlen=max_len, padding='post')\nX_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len, padding='post')\n\n# Build BiLSTM Model\nbilstm_model = Sequential([\n    Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n    Bidirectional(LSTM(64, return_sequences=False)),\n    Dropout(0.5),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nbilstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(\"\\nTraining BiLSTM Model...\")\nbilstm_model.fit(X_train_seq, y_combined, epochs=5, batch_size=32, validation_split=0.2)\n\n# Evaluate BiLSTM\nprint(\"\\nEvaluating BiLSTM Model...\")\ny_pred_bilstm = (bilstm_model.predict(X_test_seq) > 0.5).astype(int)\n\nprecision_bilstm = precision_score(y_test, y_pred_bilstm, average='macro')\nrecall_bilstm = recall_score(y_test, y_pred_bilstm, average='macro')\nf1_bilstm = f1_score(y_test, y_pred_bilstm, average='macro')\n\nprint(f\"BiLSTM Results:\")\nprint(f\"Precision: {precision_bilstm:.4f}\")\nprint(f\"Recall: {recall_bilstm:.4f}\")\nprint(f\"Macro F1 Score: {f1_bilstm:.4f}\")\n\n# Build CNN + LSTM Model\ncnn_lstm_model = Sequential([\n    Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n    Conv1D(128, 5, activation='relu'),\n    MaxPooling1D(pool_size=2),\n    LSTM(64, return_sequences=False),\n    Dropout(0.5),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\ncnn_lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(\"\\nTraining CNN + LSTM Model...\")\ncnn_lstm_model.fit(X_train_seq, y_combined, epochs=5, batch_size=32, validation_split=0.2)\n\n# Evaluate CNN + LSTM\nprint(\"\\nEvaluating CNN + LSTM Model...\")\ny_pred_cnn_lstm = (cnn_lstm_model.predict(X_test_seq) > 0.5).astype(int)\n\nprecision_cnn_lstm = precision_score(y_test, y_pred_cnn_lstm, average='macro')\nrecall_cnn_lstm = recall_score(y_test, y_pred_cnn_lstm, average='macro')\nf1_cnn_lstm = f1_score(y_test, y_pred_cnn_lstm, average='macro')\n\nprint(f\"CNN + LSTM Results:\")\nprint(f\"Precision: {precision_cnn_lstm:.4f}\")\nprint(f\"Recall: {recall_cnn_lstm:.4f}\")\nprint(f\"Macro F1 Score: {f1_cnn_lstm:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T07:44:45.352129Z","iopub.execute_input":"2025-01-29T07:44:45.352762Z","iopub.status.idle":"2025-01-29T07:44:58.957895Z","shell.execute_reply.started":"2025-01-29T07:44:45.352736Z","shell.execute_reply":"2025-01-29T07:44:58.957174Z"}},"outputs":[{"name":"stdout","text":"\nTraining BiLSTM Model...\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m85/85\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.5074 - loss: 0.6934 - val_accuracy: 0.5355 - val_loss: 0.6921\nEpoch 2/5\n\u001b[1m85/85\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5152 - loss: 0.6928 - val_accuracy: 0.5325 - val_loss: 0.6919\nEpoch 3/5\n\u001b[1m85/85\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5415 - loss: 0.6915 - val_accuracy: 0.4675 - val_loss: 0.6953\nEpoch 4/5\n\u001b[1m85/85\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5136 - loss: 0.6931 - val_accuracy: 0.5118 - val_loss: 0.6925\nEpoch 5/5\n\u001b[1m85/85\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4925 - loss: 0.6924 - val_accuracy: 0.5577 - val_loss: 0.6889\n\nEvaluating BiLSTM Model...\n\u001b[1m19/19\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\nBiLSTM Results:\nPrecision: 0.5562\nRecall: 0.5258\nMacro F1 Score: 0.4477\n\nTraining CNN + LSTM Model...\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m85/85\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.4877 - loss: 0.6932 - val_accuracy: 0.5370 - val_loss: 0.6925\nEpoch 2/5\n\u001b[1m85/85\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5077 - loss: 0.6928 - val_accuracy: 0.4675 - val_loss: 0.6926\nEpoch 3/5\n\u001b[1m85/85\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5104 - loss: 0.6937 - val_accuracy: 0.4675 - val_loss: 0.6924\nEpoch 4/5\n\u001b[1m85/85\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4902 - loss: 0.6926 - val_accuracy: 0.5355 - val_loss: 0.6912\nEpoch 5/5\n\u001b[1m85/85\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5133 - loss: 0.6907 - val_accuracy: 0.5355 - val_loss: 0.6909\n\nEvaluating CNN + LSTM Model...\n\u001b[1m19/19\u001b[0m \u001b[32mтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБтФБ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\nCNN + LSTM Results:\nPrecision: 0.2446\nRecall: 0.4983\nMacro F1 Score: 0.3281\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"**Transformers**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:11:42.392829Z","iopub.execute_input":"2025-01-29T17:11:42.393149Z","iopub.status.idle":"2025-01-29T17:11:43.039292Z","shell.execute_reply.started":"2025-01-29T17:11:42.393121Z","shell.execute_reply":"2025-01-29T17:11:43.038413Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"**mBERT**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for mBERT\nmodel_name = 'bert-base-multilingual-cased'  # Suitable for multilingual datasets like Tamil\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES\n)\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)  # Compute the loss here\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:18:55.897470Z","iopub.execute_input":"2025-01-29T17:18:55.897814Z","iopub.status.idle":"2025-01-29T17:23:18.417475Z","shell.execute_reply.started":"2025-01-29T17:18:55.897793Z","shell.execute_reply":"2025-01-29T17:23:18.416341Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"becf23bd3398469ead603f710c6ec4c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d827212ab19e4d0ebdafdb363a1c2899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05d4dd903ff4436dbeb75c068a56dd78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c6af88168284304a0f335992e4cecb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56215c7ab1fa47c68848bd22dd3ec17b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:19<00:00,  4.51it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 18.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6722 | Train Accuracy: 0.5599\nValidation Loss: 0.5661 | Validation Accuracy: 0.6973 | F1 Score: 0.6952\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:18<00:00,  4.69it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 18.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.5434 | Train Accuracy: 0.7222\nValidation Loss: 0.5364 | Validation Accuracy: 0.7341 | F1 Score: 0.7243\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:18<00:00,  4.63it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 18.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.4465 | Train Accuracy: 0.7927\nValidation Loss: 0.4956 | Validation Accuracy: 0.7458 | F1 Score: 0.7386\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:19<00:00,  4.52it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 17.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.3576 | Train Accuracy: 0.8528\nValidation Loss: 0.4642 | Validation Accuracy: 0.7893 | F1 Score: 0.7896\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:19<00:00,  4.48it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 16.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.2701 | Train Accuracy: 0.8910\nValidation Loss: 0.6343 | Validation Accuracy: 0.7726 | F1 Score: 0.7717\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:19<00:00,  4.39it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 16.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.2218 | Train Accuracy: 0.9129\nValidation Loss: 0.6000 | Validation Accuracy: 0.7860 | F1 Score: 0.7860\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:20<00:00,  4.30it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 16.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.1525 | Train Accuracy: 0.9435\nValidation Loss: 0.6212 | Validation Accuracy: 0.7876 | F1 Score: 0.7878\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:20<00:00,  4.20it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.0877 | Train Accuracy: 0.9748\nValidation Loss: 0.6826 | Validation Accuracy: 0.7826 | F1 Score: 0.7829\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:21<00:00,  4.06it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 14.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.0753 | Train Accuracy: 0.9752\nValidation Loss: 0.7049 | Validation Accuracy: 0.7910 | F1 Score: 0.7911\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:21<00:00,  4.04it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.0619 | Train Accuracy: 0.9831\nValidation Loss: 0.7449 | Validation Accuracy: 0.7793 | F1 Score: 0.7795\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:21<00:00,  4.11it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.0592 | Train Accuracy: 0.9824\nValidation Loss: 0.7419 | Validation Accuracy: 0.7809 | F1 Score: 0.7812\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:21<00:00,  4.11it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.0508 | Train Accuracy: 0.9856\nValidation Loss: 0.7455 | Validation Accuracy: 0.7809 | F1 Score: 0.7812\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n# Print the Macro F1 Score ,Test Accuracy,Precision and Recall\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:23:37.678592Z","iopub.execute_input":"2025-01-29T17:23:37.679268Z","iopub.status.idle":"2025-01-29T17:23:42.250672Z","shell.execute_reply.started":"2025-01-29T17:23:37.679236Z","shell.execute_reply":"2025-01-29T17:23:42.249930Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:04<00:00,  4.17it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7657\nTest Accuracy: 0.7659\nPrecision: 0.7658\nRecall: 0.7657\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"**Hyperparameter Tuning**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for mBERT\nmodel_name = 'bert-base-multilingual-cased'  # Suitable for multilingual datasets like Tamil\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 16\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES\n)\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)  # Compute the loss here\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 8\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:23:53.715612Z","iopub.execute_input":"2025-01-29T17:23:53.715921Z","iopub.status.idle":"2025-01-29T17:27:46.447730Z","shell.execute_reply.started":"2025-01-29T17:23:53.715899Z","shell.execute_reply":"2025-01-29T17:27:46.446844Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:27<00:00,  6.28it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 27.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/8\nTrain Loss: 0.6502 | Train Accuracy: 0.6017\nValidation Loss: 0.6007 | Validation Accuracy: 0.6555 | F1 Score: 0.6388\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:27<00:00,  6.22it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 28.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/8\nTrain Loss: 0.5345 | Train Accuracy: 0.7265\nValidation Loss: 0.5309 | Validation Accuracy: 0.7559 | F1 Score: 0.7528\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:27<00:00,  6.31it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 29.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/8\nTrain Loss: 0.4339 | Train Accuracy: 0.8078\nValidation Loss: 0.4807 | Validation Accuracy: 0.7843 | F1 Score: 0.7815\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:27<00:00,  6.30it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 28.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/8\nTrain Loss: 0.3395 | Train Accuracy: 0.8589\nValidation Loss: 0.5447 | Validation Accuracy: 0.7843 | F1 Score: 0.7845\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:27<00:00,  6.27it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 28.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/8\nTrain Loss: 0.2527 | Train Accuracy: 0.8992\nValidation Loss: 0.5335 | Validation Accuracy: 0.7709 | F1 Score: 0.7706\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:27<00:00,  6.30it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 28.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/8\nTrain Loss: 0.1638 | Train Accuracy: 0.9403\nValidation Loss: 0.7809 | Validation Accuracy: 0.7692 | F1 Score: 0.7694\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:27<00:00,  6.33it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 29.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/8\nTrain Loss: 0.0798 | Train Accuracy: 0.9719\nValidation Loss: 0.8181 | Validation Accuracy: 0.7726 | F1 Score: 0.7727\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:27<00:00,  6.34it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 29.16it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 8/8\nTrain Loss: 0.0547 | Train Accuracy: 0.9852\nValidation Loss: 0.8840 | Validation Accuracy: 0.7676 | F1 Score: 0.7678\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n# Print the Macro F1 Score ,Test Accuracy,Precision and Recall\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:27:50.571544Z","iopub.execute_input":"2025-01-29T17:27:50.571885Z","iopub.status.idle":"2025-01-29T17:27:55.664007Z","shell.execute_reply.started":"2025-01-29T17:27:50.571860Z","shell.execute_reply":"2025-01-29T17:27:55.663098Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:05<00:00,  7.49it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7686\nTest Accuracy: 0.7692\nPrecision: 0.7704\nRecall: 0.7685\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"**XLM-RoBERTa**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for XLM-RoBERTa\nmodel_name = 'xlm-roberta-base'  # Switching to XLM-RoBERTa\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES\n)\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)  # Compute the loss here\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:31:08.144003Z","iopub.execute_input":"2025-01-29T17:31:08.144393Z","iopub.status.idle":"2025-01-29T17:36:56.275509Z","shell.execute_reply.started":"2025-01-29T17:31:08.144363Z","shell.execute_reply":"2025-01-29T17:36:56.274494Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"542993e39e534e0c839c16be7202c1e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e2bda490d024e2da8dfedbc3e1a42a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b902b743bc4eb5a9291f07deda55ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccf2b1b372044c1ebc7cc6487cf705b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caa162ef2fbf400f8ee989675c9f3284"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:25<00:00,  3.38it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 14.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6903 | Train Accuracy: 0.5264\nValidation Loss: 0.6403 | Validation Accuracy: 0.6371 | F1 Score: 0.6241\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:26<00:00,  3.27it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.5866 | Train Accuracy: 0.6931\nValidation Loss: 0.4898 | Validation Accuracy: 0.7726 | F1 Score: 0.7723\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.16it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 12.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.5250 | Train Accuracy: 0.7499\nValidation Loss: 0.4735 | Validation Accuracy: 0.7692 | F1 Score: 0.7692\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:26<00:00,  3.23it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.4415 | Train Accuracy: 0.8021\nValidation Loss: 0.4675 | Validation Accuracy: 0.7910 | F1 Score: 0.7911\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:26<00:00,  3.24it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.3860 | Train Accuracy: 0.8377\nValidation Loss: 0.5131 | Validation Accuracy: 0.7793 | F1 Score: 0.7793\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.21it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.3436 | Train Accuracy: 0.8636\nValidation Loss: 0.5002 | Validation Accuracy: 0.7993 | F1 Score: 0.7987\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.21it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.2844 | Train Accuracy: 0.8942\nValidation Loss: 0.6060 | Validation Accuracy: 0.7793 | F1 Score: 0.7789\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:26<00:00,  3.22it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.2055 | Train Accuracy: 0.9259\nValidation Loss: 0.6172 | Validation Accuracy: 0.7759 | F1 Score: 0.7759\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:26<00:00,  3.22it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.1902 | Train Accuracy: 0.9367\nValidation Loss: 0.6415 | Validation Accuracy: 0.7843 | F1 Score: 0.7841\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:26<00:00,  3.22it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.1731 | Train Accuracy: 0.9395\nValidation Loss: 0.6655 | Validation Accuracy: 0.7826 | F1 Score: 0.7823\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.20it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.1709 | Train Accuracy: 0.9406\nValidation Loss: 0.6694 | Validation Accuracy: 0.7843 | F1 Score: 0.7839\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.20it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.37it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.1747 | Train Accuracy: 0.9431\nValidation Loss: 0.6682 | Validation Accuracy: 0.7793 | F1 Score: 0.7790\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n# Print the Macro F1 Score, Test Accuracy, and Confusion Matrix\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\n\n# Plotting the confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:37:49.348853Z","iopub.execute_input":"2025-01-29T17:37:49.349162Z","iopub.status.idle":"2025-01-29T17:37:53.827876Z","shell.execute_reply.started":"2025-01-29T17:37:49.349138Z","shell.execute_reply":"2025-01-29T17:37:53.826851Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:04<00:00,  4.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.8009\nTest Accuracy: 0.8010\nPrecision: 0.8026\nRecall: 0.8017\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"**Hyperparameter Tuning**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for XLM-RoBERTa\nmodel_name = 'xlm-roberta-base'  # Switching to XLM-RoBERTa\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 16\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES\n)\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)  # Compute the loss here\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 8\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:38:05.102895Z","iopub.execute_input":"2025-01-29T17:38:05.103249Z","iopub.status.idle":"2025-01-29T17:43:17.200235Z","shell.execute_reply.started":"2025-01-29T17:38:05.103210Z","shell.execute_reply":"2025-01-29T17:43:17.199318Z"}},"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:37<00:00,  4.69it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 24.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/8\nTrain Loss: 0.6960 | Train Accuracy: 0.5038\nValidation Loss: 0.6876 | Validation Accuracy: 0.5753 | F1 Score: 0.5319\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:37<00:00,  4.65it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 25.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/8\nTrain Loss: 0.6664 | Train Accuracy: 0.6078\nValidation Loss: 0.5980 | Validation Accuracy: 0.6572 | F1 Score: 0.6331\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:37<00:00,  4.69it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 25.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/8\nTrain Loss: 0.5896 | Train Accuracy: 0.7017\nValidation Loss: 0.5490 | Validation Accuracy: 0.7475 | F1 Score: 0.7478\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:37<00:00,  4.65it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 25.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/8\nTrain Loss: 0.5367 | Train Accuracy: 0.7431\nValidation Loss: 0.6180 | Validation Accuracy: 0.7207 | F1 Score: 0.7061\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:37<00:00,  4.68it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 25.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/8\nTrain Loss: 0.4762 | Train Accuracy: 0.7852\nValidation Loss: 0.5272 | Validation Accuracy: 0.7726 | F1 Score: 0.7718\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:36<00:00,  4.70it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 25.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/8\nTrain Loss: 0.4172 | Train Accuracy: 0.8176\nValidation Loss: 0.5160 | Validation Accuracy: 0.7876 | F1 Score: 0.7879\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:37<00:00,  4.69it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 25.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/8\nTrain Loss: 0.3689 | Train Accuracy: 0.8435\nValidation Loss: 0.5760 | Validation Accuracy: 0.7726 | F1 Score: 0.7725\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 174/174 [00:37<00:00,  4.68it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:01<00:00, 25.01it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 8/8\nTrain Loss: 0.3227 | Train Accuracy: 0.8687\nValidation Loss: 0.5260 | Validation Accuracy: 0.8077 | F1 Score: 0.8070\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n# Print the Macro F1 Score, Test Accuracy, and Confusion Matrix\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\n\n# Plotting the confusion matrix as a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:43:52.036815Z","iopub.execute_input":"2025-01-29T17:43:52.037150Z","iopub.status.idle":"2025-01-29T17:43:56.808748Z","shell.execute_reply.started":"2025-01-29T17:43:52.037125Z","shell.execute_reply":"2025-01-29T17:43:56.807669Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 38/38 [00:04<00:00,  8.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7773\nTest Accuracy: 0.7776\nPrecision: 0.7808\nRecall: 0.7786\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Store sample texts, true labels, and predictions\nsample_texts = []  \nall_preds = []\nall_targets = []\n\n# Iterate over the test set to collect predictions\nwith torch.no_grad():\n    for batch_idx, batch in enumerate(test_loader):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets'].to(device)\n\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, 1)\n\n        # Store predictions and actual labels\n        all_preds.extend(predicted.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n\n        # Collect the 'cleanText' for the current batch from the test dataframe\n        start_idx = batch_idx * BATCH_SIZE\n        end_idx = start_idx + len(batch['ids'])\n        sample_texts.extend(test_df['cleanText'].iloc[start_idx:end_idx].tolist())\n\n# Randomly select some samples to display\nnum_samples = 5  # Number of examples to display\nsample_indices = np.random.choice(len(all_targets), num_samples, replace=False)\n\n# Prepare the sample data for display\nsample_data = {\n    \"Text\": [sample_texts[i] for i in sample_indices],\n    \"Actual Label and Predicted Label\": [f\"{all_targets[i]} -> {all_preds[i]}\" for i in sample_indices]\n}\n\n# Create a DataFrame\nsample_df = pd.DataFrame(sample_data)\n\n# Display the DataFrame with the format \"Actual Label -> Predicted Label\"\nprint(\"\\nSample Predictions:\")\nprint(sample_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T10:32:53.662707Z","iopub.execute_input":"2025-01-29T10:32:53.663053Z","iopub.status.idle":"2025-01-29T10:32:57.411812Z","shell.execute_reply.started":"2025-01-29T10:32:53.663025Z","shell.execute_reply":"2025-01-29T10:32:57.410853Z"}},"outputs":[{"name":"stdout","text":"\nSample Predictions:\n                                                Text  \\\n0  роЪроЪрпНроЪ родро┐ро╡рпНропро╛ роорпБройрпН роЯро╛ роорпЗ роЙрогрпНроорпИропро┐ро▓рпН роЕроирпНродрокрпН рокро░роЯрпНроЯрпИ...   \n1  роЙройрпНройрпЛроЯ роЖроЪрпИроХрпНроХро╛роХ рокрпЖродрпНродро╡роЩрпНроХро│ роХро╖рпНроЯ рокроЯрпБродрпНродро╛род роирпАродро╛рой...   \n2  рокрпЖро░ро┐роп ро╣рпАро░рпЛропро┐ройрпН роХрпВроЯ роЗрокрпНрокро┐роЯро┐ ро╕рпЗро▓рпНроГрокро┐роП роПродрпБроХрпНроХ роородрпН...   \n3  роЙроЩрпНроХ рокро╛ро▓рпКро╡ро░рпНроЪрпИ рокро╛ро░рпНроХрпНроХрпБроорпН рокрпЛродрпБ родрпЖро░ро┐ропрпБродрпБ ро╖роХро┐ро▓ро╛ро╡...   \n4  роЗрокрпНрокроЯро┐ роТро░рпБ рокрпЖрогрпН роЗро░рпБроХрпНроХрпБро▒родрпБроХрпНроХрпБ роЗро▓рпНро▓ро╛роо роЗро░рпБроХрпНроХро▓ро╛...   \n\n  Actual Label and Predicted Label  \n0                           1 -> 0  \n1                           1 -> 1  \n2                           0 -> 0  \n3                           1 -> 1  \n4                           1 -> 1  \n","output_type":"stream"}],"execution_count":152},{"cell_type":"markdown","source":"**Indic Sentence-BERT**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer and model for Indic Sentence-BERT\nmodel_name = 'l3cube-pune/indic-sentence-bert-nli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Set `num_labels` for binary classification\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Loss and optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Mixed-precision training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:44:11.575108Z","iopub.execute_input":"2025-01-29T17:44:11.575459Z","iopub.status.idle":"2025-01-29T17:49:10.374373Z","shell.execute_reply.started":"2025-01-29T17:44:11.575432Z","shell.execute_reply":"2025-01-29T17:49:10.373530Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"063b87d1609a44ec8309dd34332378d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b73a8e2cb9947c9a753f361ffde89ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98278f52987642b48a13ec2e66b9af92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ebc08a6e8b454089c14219571449e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/668 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fc5d5e8a8584b0a93feb1a88dcec3a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/950M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb453f95ef748a7b852ec1952d62f17"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at l3cube-pune/indic-sentence-bert-nli and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.86it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6860 | Train Accuracy: 0.6402\nValidation Loss: 0.6658 | Validation Accuracy: 0.6839 | F1 Score: 0.6629\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:23<00:00,  3.69it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 14.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.6150 | Train Accuracy: 0.7600\nValidation Loss: 0.5654 | Validation Accuracy: 0.7642 | F1 Score: 0.7645\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:23<00:00,  3.73it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.5138 | Train Accuracy: 0.8165\nValidation Loss: 0.5199 | Validation Accuracy: 0.7676 | F1 Score: 0.7670\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.4267 | Train Accuracy: 0.8510\nValidation Loss: 0.4895 | Validation Accuracy: 0.7826 | F1 Score: 0.7817\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:23<00:00,  3.78it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.3502 | Train Accuracy: 0.8856\nValidation Loss: 0.4735 | Validation Accuracy: 0.8027 | F1 Score: 0.8027\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:23<00:00,  3.75it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.2947 | Train Accuracy: 0.9064\nValidation Loss: 0.5124 | Validation Accuracy: 0.7876 | F1 Score: 0.7872\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.2548 | Train Accuracy: 0.9219\nValidation Loss: 0.5091 | Validation Accuracy: 0.8027 | F1 Score: 0.8027\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.2097 | Train Accuracy: 0.9410\nValidation Loss: 0.5190 | Validation Accuracy: 0.8194 | F1 Score: 0.8188\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:23<00:00,  3.78it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.1804 | Train Accuracy: 0.9514\nValidation Loss: 0.5690 | Validation Accuracy: 0.7843 | F1 Score: 0.7842\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.78it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.1654 | Train Accuracy: 0.9532\nValidation Loss: 0.5501 | Validation Accuracy: 0.8094 | F1 Score: 0.8095\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:23<00:00,  3.77it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.1401 | Train Accuracy: 0.9593\nValidation Loss: 0.5789 | Validation Accuracy: 0.7960 | F1 Score: 0.7962\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:23<00:00,  3.75it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.38it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.1131 | Train Accuracy: 0.9701\nValidation Loss: 0.5865 | Validation Accuracy: 0.8027 | F1 Score: 0.8029\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\n\n# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n# Print the Macro F1 Score and Test Accuracy\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:49:36.998025Z","iopub.execute_input":"2025-01-29T17:49:36.998366Z","iopub.status.idle":"2025-01-29T17:49:41.432850Z","shell.execute_reply.started":"2025-01-29T17:49:36.998323Z","shell.execute_reply":"2025-01-29T17:49:41.431892Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:04<00:00,  4.30it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7684\nTest Accuracy: 0.7692\nPrecision: 0.7710\nRecall: 0.7683\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"**DistilBERT-mc**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer and model for distilbert-base-multilingual-cased\nmodel_name = 'distilbert-base-multilingual-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Set `num_labels` for binary classification\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Loss and optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Mixed-precision training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:49:53.380935Z","iopub.execute_input":"2025-01-29T17:49:53.381438Z","iopub.status.idle":"2025-01-29T17:52:47.093167Z","shell.execute_reply.started":"2025-01-29T17:49:53.381383Z","shell.execute_reply":"2025-01-29T17:52:47.092378Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49d6f15f30884e4cb32611b354170e2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c47c4c5b092140ada6be56cd9fe41ab6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53cd0f8f4c384878aa9bf6daaed469a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"894bd438bef4478aa98e73bb50cc40cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8eab3769e9a443b9fecc7ce46332858"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.60it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 25.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6790 | Train Accuracy: 0.5603\nValidation Loss: 0.6099 | Validation Accuracy: 0.6555 | F1 Score: 0.6556\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.54it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 24.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5612 | Train Accuracy: 0.7107\nValidation Loss: 0.5168 | Validation Accuracy: 0.7559 | F1 Score: 0.7561\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.44it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 24.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5059 | Train Accuracy: 0.7503\nValidation Loss: 0.5284 | Validation Accuracy: 0.7375 | F1 Score: 0.7340\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.41it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 25.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4430 | Train Accuracy: 0.7978\nValidation Loss: 0.4906 | Validation Accuracy: 0.7575 | F1 Score: 0.7578\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.53it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 25.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3845 | Train Accuracy: 0.8356\nValidation Loss: 0.5364 | Validation Accuracy: 0.7542 | F1 Score: 0.7513\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.60it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 25.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3361 | Train Accuracy: 0.8611\nValidation Loss: 0.5196 | Validation Accuracy: 0.7575 | F1 Score: 0.7577\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.59it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 25.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2782 | Train Accuracy: 0.8852\nValidation Loss: 0.6017 | Validation Accuracy: 0.7525 | F1 Score: 0.7526\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.58it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 25.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2216 | Train Accuracy: 0.9104\nValidation Loss: 0.6728 | Validation Accuracy: 0.7391 | F1 Score: 0.7350\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.54it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 24.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1972 | Train Accuracy: 0.9205\nValidation Loss: 0.6785 | Validation Accuracy: 0.7341 | F1 Score: 0.7341\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.54it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 25.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1297 | Train Accuracy: 0.9550\nValidation Loss: 0.7890 | Validation Accuracy: 0.7441 | F1 Score: 0.7444\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.55it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 24.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1021 | Train Accuracy: 0.9644\nValidation Loss: 0.8548 | Validation Accuracy: 0.7542 | F1 Score: 0.7539\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:13<00:00,  6.59it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:00<00:00, 25.12it/s]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0845 | Train Accuracy: 0.9673\nValidation Loss: 0.9205 | Validation Accuracy: 0.7408 | F1 Score: 0.7411\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\n\n# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the Macro F1 Score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the Test Accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n\n# Print the results\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:56:31.821597Z","iopub.execute_input":"2025-01-29T17:56:31.821947Z","iopub.status.idle":"2025-01-29T17:56:34.071749Z","shell.execute_reply.started":"2025-01-29T17:56:31.821925Z","shell.execute_reply":"2025-01-29T17:56:34.070853Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:02<00:00,  8.51it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7485\nTest Accuracy: 0.7492\nPrecision: 0.7500\nRecall: 0.7485\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"**DeOffXLMR-Tamil**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer and model name\nmodel_name = 'Hate-speech-CNERG/deoffxlmr-mono-tamil'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets (replace train_df and dev_df with your actual data)\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES ,ignore_mismatched_sizes=True\n)\n\n# Resize the classification head to match NUM_CLASSES\nmodel.classifier.out_proj = torch.nn.Linear(model.config.hidden_size, NUM_CLASSES)\n\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T17:56:39.693757Z","iopub.execute_input":"2025-01-29T17:56:39.694090Z","iopub.status.idle":"2025-01-29T18:02:40.118518Z","shell.execute_reply.started":"2025-01-29T17:56:39.694060Z","shell.execute_reply":"2025-01-29T18:02:40.117697Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/211 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58da4cf639034f9692a396bab90d376a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3689240ed560426db6cfc426fd7b678a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ff7c66f8fe340ca96716725b90f3f79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51594808dd27400f90db1848e8c762d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a3fcb5c8f154bfb83c0afc090361a1c"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Hate-speech-CNERG/deoffxlmr-mono-tamil and are newly initialized because the shapes did not match:\n- classifier.out_proj.weight: found shape torch.Size([6, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n- classifier.out_proj.bias: found shape torch.Size([6]) in the checkpoint and torch.Size([2]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:25<00:00,  3.38it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 14.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6288 | Train Accuracy: 0.6387\nValidation Loss: 0.5536 | Validation Accuracy: 0.7358 | F1 Score: 0.7314\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:26<00:00,  3.28it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.4935 | Train Accuracy: 0.7686\nValidation Loss: 0.5054 | Validation Accuracy: 0.7559 | F1 Score: 0.7560\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.17it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 12.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.3892 | Train Accuracy: 0.8381\nValidation Loss: 0.5108 | Validation Accuracy: 0.7759 | F1 Score: 0.7759\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.20it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.3167 | Train Accuracy: 0.8723\nValidation Loss: 0.5238 | Validation Accuracy: 0.7692 | F1 Score: 0.7694\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:26<00:00,  3.24it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.2256 | Train Accuracy: 0.9162\nValidation Loss: 0.6295 | Validation Accuracy: 0.7676 | F1 Score: 0.7651\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.20it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.1509 | Train Accuracy: 0.9539\nValidation Loss: 0.6488 | Validation Accuracy: 0.7709 | F1 Score: 0.7709\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.22it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.1405 | Train Accuracy: 0.9547\nValidation Loss: 0.6599 | Validation Accuracy: 0.7793 | F1 Score: 0.7791\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.22it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.1256 | Train Accuracy: 0.9590\nValidation Loss: 0.6828 | Validation Accuracy: 0.7776 | F1 Score: 0.7777\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.22it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.1164 | Train Accuracy: 0.9601\nValidation Loss: 0.6835 | Validation Accuracy: 0.7759 | F1 Score: 0.7759\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.21it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.1132 | Train Accuracy: 0.9647\nValidation Loss: 0.6862 | Validation Accuracy: 0.7776 | F1 Score: 0.7776\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.21it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.1104 | Train Accuracy: 0.9651\nValidation Loss: 0.6883 | Validation Accuracy: 0.7759 | F1 Score: 0.7759\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:27<00:00,  3.22it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.51it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.1165 | Train Accuracy: 0.9622\nValidation Loss: 0.6885 | Validation Accuracy: 0.7759 | F1 Score: 0.7759\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the Macro F1 Score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the Test Accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n\n# Print the results\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:02:44.362233Z","iopub.execute_input":"2025-01-29T18:02:44.362554Z","iopub.status.idle":"2025-01-29T18:02:48.745645Z","shell.execute_reply.started":"2025-01-29T18:02:44.362531Z","shell.execute_reply":"2025-01-29T18:02:48.744773Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:04<00:00,  4.35it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7675\nTest Accuracy: 0.7676\nPrecision: 0.7675\nRecall: 0.7676\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"**Tamil BERT**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer for Tamil BERT\nmodel_name = 'ai4bharat/indic-bert'  # Tamil-specific pre-trained model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\nNUM_CLASSES = 2  # Update based on your dataset (e.g., positive/negative sentiment)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES\n)\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)  # Compute the loss here\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:02:52.907044Z","iopub.execute_input":"2025-01-29T18:02:52.907337Z","iopub.status.idle":"2025-01-29T18:06:48.168843Z","shell.execute_reply.started":"2025-01-29T18:02:52.907316Z","shell.execute_reply":"2025-01-29T18:06:48.167946Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2f5927775e4185aea70cf6dd4e585e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0fc91e0e3f4d3fbfdc291df49f56ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b805d978ec64cc5bc59af2ec34164ae"}},"metadata":{}},{"name":"stderr","text":"Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:17<00:00,  4.93it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6930 | Train Accuracy: 0.5121\nValidation Loss: 0.6925 | Validation Accuracy: 0.5351 | F1 Score: 0.3731\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:18<00:00,  4.74it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 12.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.6926 | Train Accuracy: 0.5257\nValidation Loss: 0.6924 | Validation Accuracy: 0.5468 | F1 Score: 0.4194\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:18<00:00,  4.82it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.6917 | Train Accuracy: 0.5621\nValidation Loss: 0.6914 | Validation Accuracy: 0.5803 | F1 Score: 0.5756\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:17<00:00,  4.96it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.6887 | Train Accuracy: 0.5952\nValidation Loss: 0.6890 | Validation Accuracy: 0.5719 | F1 Score: 0.5693\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:17<00:00,  4.96it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.6800 | Train Accuracy: 0.6438\nValidation Loss: 0.6819 | Validation Accuracy: 0.6020 | F1 Score: 0.6008\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:17<00:00,  4.90it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.6661 | Train Accuracy: 0.6808\nValidation Loss: 0.6757 | Validation Accuracy: 0.6037 | F1 Score: 0.6032\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:17<00:00,  4.86it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.6510 | Train Accuracy: 0.6920\nValidation Loss: 0.6669 | Validation Accuracy: 0.6438 | F1 Score: 0.6377\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:17<00:00,  4.92it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.6302 | Train Accuracy: 0.7154\nValidation Loss: 0.6609 | Validation Accuracy: 0.6555 | F1 Score: 0.6541\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:17<00:00,  4.94it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.6098 | Train Accuracy: 0.7409\nValidation Loss: 0.6549 | Validation Accuracy: 0.6555 | F1 Score: 0.6473\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:17<00:00,  4.94it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.5914 | Train Accuracy: 0.7517\nValidation Loss: 0.6828 | Validation Accuracy: 0.6421 | F1 Score: 0.6415\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:17<00:00,  4.91it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.5682 | Train Accuracy: 0.7744\nValidation Loss: 0.6589 | Validation Accuracy: 0.6589 | F1 Score: 0.6568\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:17<00:00,  4.90it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 13.21it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.5531 | Train Accuracy: 0.7845\nValidation Loss: 0.6819 | Validation Accuracy: 0.6555 | F1 Score: 0.6559\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(input_ids, attention_mask=attention_mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the Macro F1 Score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the Test Accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n\n# Print the results\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:07:36.112134Z","iopub.execute_input":"2025-01-29T18:07:36.112461Z","iopub.status.idle":"2025-01-29T18:07:40.737109Z","shell.execute_reply.started":"2025-01-29T18:07:36.112438Z","shell.execute_reply":"2025-01-29T18:07:40.736175Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:04<00:00,  4.13it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.6789\nTest Accuracy: 0.6806\nPrecision: 0.6821\nRecall: 0.6794\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"**MuRIL**","metadata":{}},{"cell_type":"code","source":"# Set random seed for reproducibility\nseed_value = 42\nnp.random.seed(seed_value)\ntorch.manual_seed(seed_value)\ntorch.cuda.manual_seed_all(seed_value)\n\n# Define tokenizer and model name\nmodel_name = 'google/muril-base-cased'  # Changed model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Constants\nMAX_LEN = 128\nBATCH_SIZE = 32\nNUM_CLASSES = 2  # Update based on your dataset\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Custom Dataset Class\nclass TamilDataset(Dataset):\n    def __init__(self, texts, labels=None, tokenizer=None, max_len=None, is_labeled=False):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.is_labeled = is_labeled\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n\n        if self.is_labeled:\n            label = self.labels[item]\n            return {\n                'ids': input_ids,\n                'mask': attention_mask,\n                'targets': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'ids': input_ids,\n                'mask': attention_mask\n            }\n\n# Prepare datasets (replace train_df and dev_df with your actual data)\ntrain_set = TamilDataset(train_df['cleanText'].tolist(), train_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ndev_set = TamilDataset(dev_df['cleanText'].tolist(), dev_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\ndev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)\n\ntest_set = TamilDataset(test_df['cleanText'].tolist(), test_df['enc_label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN, is_labeled=True)\ntest_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n\n# Define model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n)\n\n# Resize the classification head to match NUM_CLASSES\nmodel.classifier.out_proj = torch.nn.Linear(model.config.hidden_size, NUM_CLASSES)\n\nmodel.to(device)\n\n# Define loss function (CrossEntropyLoss)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n\n# Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n\n# Mixed-Precision Training\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\ndef train_model():\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n        optimizer.zero_grad()\n\n        input_ids = batch['ids'].to(device)\n        attention_mask = batch['mask'].to(device)\n        labels = batch['targets'].to(device)\n\n        with torch.cuda.amp.autocast():  # Mixed-precision\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)\n\n        scaler.scale(loss).backward()  # Backpropagate gradients\n        scaler.step(optimizer)  # Update weights\n        scaler.update()  # Update the scaler\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(outputs.logits, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    return total_loss / len(train_loader), accuracy\n\n# Validation loop\ndef evaluate_model():\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(dev_loader, desc=\"Validation Batches\"):\n            input_ids = batch['ids'].to(device)\n            attention_mask = batch['mask'].to(device)\n            labels = batch['targets'].to(device)\n\n            with torch.cuda.amp.autocast():  # Mixed-precision\n                outputs = model(input_ids, attention_mask=attention_mask)\n                loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return total_loss / len(dev_loader), accuracy, f1\n\n# Training and evaluation loop\nEPOCHS = 12\nfor epoch in range(EPOCHS):\n    train_loss, train_accuracy = train_model()\n    val_loss, val_accuracy, val_f1 = evaluate_model()\n\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_accuracy:.4f} | F1 Score: {val_f1:.4f}\")\n\n    # Learning rate scheduler\n    scheduler.step(val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:07:45.107257Z","iopub.execute_input":"2025-01-29T18:07:45.107636Z","iopub.status.idle":"2025-01-29T18:12:43.774680Z","shell.execute_reply.started":"2025-01-29T18:07:45.107608Z","shell.execute_reply":"2025-01-29T18:12:43.773744Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fe67dcd13a949dd8d66fa8ee793f08f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88737626a2274caa9381cdf54f71d790"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d994672b818462ab438ae7583c3957e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/113 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a9d188f34746e4b252ba40b4bdf4a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/953M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfd55dd406334146902330956bd0a9ed"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/muril-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.84it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/12\nTrain Loss: 0.6921 | Train Accuracy: 0.5480\nValidation Loss: 0.6856 | Validation Accuracy: 0.6873 | F1 Score: 0.6823\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:23<00:00,  3.73it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/12\nTrain Loss: 0.6481 | Train Accuracy: 0.7344\nValidation Loss: 0.6152 | Validation Accuracy: 0.7191 | F1 Score: 0.7168\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:23<00:00,  3.75it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/12\nTrain Loss: 0.5633 | Train Accuracy: 0.7895\nValidation Loss: 0.5619 | Validation Accuracy: 0.7492 | F1 Score: 0.7450\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.82it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 16.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/12\nTrain Loss: 0.4875 | Train Accuracy: 0.8215\nValidation Loss: 0.5186 | Validation Accuracy: 0.7625 | F1 Score: 0.7626\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/12\nTrain Loss: 0.4086 | Train Accuracy: 0.8600\nValidation Loss: 0.5012 | Validation Accuracy: 0.7776 | F1 Score: 0.7779\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:23<00:00,  3.78it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/12\nTrain Loss: 0.3342 | Train Accuracy: 0.8888\nValidation Loss: 0.5111 | Validation Accuracy: 0.7793 | F1 Score: 0.7792\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 16.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/12\nTrain Loss: 0.3005 | Train Accuracy: 0.8956\nValidation Loss: 0.5515 | Validation Accuracy: 0.7692 | F1 Score: 0.7679\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.82it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 16.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/12\nTrain Loss: 0.2335 | Train Accuracy: 0.9295\nValidation Loss: 0.6014 | Validation Accuracy: 0.7609 | F1 Score: 0.7601\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.82it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/12\nTrain Loss: 0.1940 | Train Accuracy: 0.9485\nValidation Loss: 0.5977 | Validation Accuracy: 0.7592 | F1 Score: 0.7595\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.80it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/12\nTrain Loss: 0.1786 | Train Accuracy: 0.9547\nValidation Loss: 0.6074 | Validation Accuracy: 0.7625 | F1 Score: 0.7628\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.79it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/12\nTrain Loss: 0.1729 | Train Accuracy: 0.9565\nValidation Loss: 0.6188 | Validation Accuracy: 0.7609 | F1 Score: 0.7612\n","output_type":"stream"},{"name":"stderr","text":"Training Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 87/87 [00:22<00:00,  3.79it/s]\nValidation Batches: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:01<00:00, 15.85it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12/12\nTrain Loss: 0.1640 | Train Accuracy: 0.9608\nValidation Loss: 0.6200 | Validation Accuracy: 0.7592 | F1 Score: 0.7595\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"# Assuming 'test_df' contains the true labels for the test set\ntrue_labels = test_df['enc_label'].tolist()  # Replace 'enc_label' with the actual column name for true labels\n\n# Generate predictions\npredictions = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n\n        # Get model outputs\n        outputs = model(ids, attention_mask=mask)\n        _, predicted = torch.max(outputs.logits, dim=1)\n        predictions.extend(predicted.cpu().numpy())\n\n# Calculate the macro F1 score\nmacro_f1 = f1_score(true_labels, predictions, average='macro')\n\n# Calculate the test accuracy\ntest_accuracy = accuracy_score(true_labels, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(true_labels, predictions, average='macro')\nrecall = recall_score(true_labels, predictions, average='macro')\n\n\n# Print the results\nprint(f\"Macro F1 Score: {macro_f1:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-29T18:13:25.400083Z","iopub.execute_input":"2025-01-29T18:13:25.400480Z","iopub.status.idle":"2025-01-29T18:13:29.820636Z","shell.execute_reply.started":"2025-01-29T18:13:25.400446Z","shell.execute_reply":"2025-01-29T18:13:29.819754Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions: 100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 19/19 [00:04<00:00,  4.31it/s]","output_type":"stream"},{"name":"stdout","text":"Macro F1 Score: 0.7723\nTest Accuracy: 0.7726\nPrecision: 0.7727\nRecall: 0.7722\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":43}]}